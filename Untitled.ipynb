{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T01:37:19.384273Z",
     "start_time": "2020-06-16T01:37:15.169165Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "# from tensorboardX import SummaryWriter\n",
    "import shutil\n",
    "import argparse\n",
    "import logging\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import make_grid\n",
    "import pretrainedmodels\n",
    "\n",
    "from networks.models import DenseNet121,DenseNet161\n",
    "from utils import losses, ramps\n",
    "from utils.metrics import compute_AUCs\n",
    "from utils.metric_logger import MetricLogger\n",
    "from dataloaders import  dataset\n",
    "from dataloaders import chest_xray_14\n",
    "from dataloaders.dataset import TwoStreamBatchSampler\n",
    "from utils.util import get_timestamp\n",
    "from validation import epochVal, epochVal_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-10T08:38:59.689821Z",
     "start_time": "2020-06-10T08:38:59.625663Z"
    }
   },
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize([0.605, 0.605, 0.605],\n",
    "                                     [0.156, 0.156, 0.156])\n",
    "train_dataset = dataset.CheXpertDataset(root_dir='../dataset/hip_4cls/training_data/',\n",
    "                                        csv_file='../dataset/hip_4cls/training_sample_single_class.csv',\n",
    "                                        transform=dataset.TransformTwice(transforms.Compose([\n",
    "                                            transforms.Resize((224, 224)),\n",
    "                                            transforms.RandomAffine(degrees=10, translate=(0.02, 0.02)),\n",
    "                                            transforms.RandomHorizontalFlip(),\n",
    "                                            # transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),\n",
    "                                            # transforms.RandomRotation(10),\n",
    "                                            # transforms.RandomResizedCrop(224),\n",
    "                                            transforms.ToTensor(),\n",
    "                                            normalize,\n",
    "                                        ])))\n",
    "print(\"train_dataset len:\",len(train_dataset))\n",
    "train_dataset_num = len(train_dataset)\n",
    "labeled_num = int(train_dataset_num*0.2)\n",
    "print(\"labeled_num:\",labeled_num)\n",
    "labeled_idxs = list(range(labeled_num))unlabeled_idxs = list(range(labeled_num, train_dataset_num))\n",
    "batch_size = 16\n",
    "labeled_bs = 4\n",
    "batch_sampler = TwoStreamBatchSampler(labeled_idxs, unlabeled_idxs, batch_size, batch_size-labeled_bs)\n",
    "def worker_init_fn(worker_id):\n",
    "    random.seed(20000+worker_id)\n",
    "train_dataloader = DataLoader(dataset=train_dataset, batch_sampler=batch_sampler,\n",
    "                                  num_workers=8, pin_memory=True, worker_init_fn=worker_init_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-10T08:38:59.689821Z",
     "start_time": "2020-06-10T08:38:59.625663Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-10T08:39:58.708025Z",
     "start_time": "2020-06-10T08:39:54.907924Z"
    }
   },
   "outputs": [],
   "source": [
    "label_count = torch.LongTensor([-1])\n",
    "for i, (_,_, (image_batch, ema_image_batch), label_batch) in enumerate(train_dataloader):\n",
    "    print(image_batch[labeled_bs:].shape)\n",
    "    \n",
    "    time2 = time.time()\n",
    "    label_count = torch.cat((label_count, torch.argmax(label_batch[:labeled_bs], dim=1)), 0)\n",
    "    break\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-10T02:36:27.365213Z",
     "start_time": "2020-06-10T02:36:27.340280Z"
    }
   },
   "outputs": [],
   "source": [
    "total = [0,0,0,0]\n",
    "for i in range(1,len(label_count)):\n",
    "    label = label_count[i]\n",
    "    total[label]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-10T02:36:36.639352Z",
     "start_time": "2020-06-10T02:36:36.630716Z"
    }
   },
   "outputs": [],
   "source": [
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-10T02:36:27.494318Z",
     "start_time": "2020-06-10T02:36:27.489907Z"
    }
   },
   "outputs": [],
   "source": [
    "def _l2_normalize(d):\n",
    "    d_reshaped = d.view(d.shape[0], -1, *(1 for _ in range(d.dim() - 2)))\n",
    "    print(\"d_reshaped:\",d_reshaped.shape)\n",
    "    d /= torch.norm(d_reshaped, dim=1, keepdim=True) + 1e-8\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T09:14:03.362419Z",
     "start_time": "2020-06-09T09:14:03.349816Z"
    }
   },
   "outputs": [],
   "source": [
    "d = torch.rand([4,4,3,3]).sub(0.5)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T09:14:10.209953Z",
     "start_time": "2020-06-09T09:14:10.193177Z"
    }
   },
   "outputs": [],
   "source": [
    "_l2_normalize(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T12:27:33.609898Z",
     "start_time": "2020-05-29T12:27:33.589856Z"
    }
   },
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize([0.605, 0.605, 0.605],\n",
    "                                     [0.156, 0.156, 0.156])\n",
    "test_dataset = dataset.CheXpertDataset(root_dir='../dataset/hip/training_data/',\n",
    "                                      csv_file='../dataset/hip/testing_fold1.csv',\n",
    "                                      transform=transforms.Compose([\n",
    "                                          transforms.Resize((256, 256)),\n",
    "                                          transforms.ToTensor(),\n",
    "                                          normalize,\n",
    "                                      ]))\n",
    "test_dataloader = DataLoader(dataset=test_dataset, batch_size=1,\n",
    "                                shuffle=True, num_workers=8, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T17:17:32.732539Z",
     "start_time": "2020-05-29T17:17:32.700748Z"
    }
   },
   "outputs": [],
   "source": [
    "a = [1,2,10]\n",
    "test_dataset[200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T12:40:14.743461Z",
     "start_time": "2020-05-29T12:40:14.217113Z"
    }
   },
   "outputs": [],
   "source": [
    "a = torch.load(\"../model/0528_hip_test_label_rate09/checkpoint/epoch_12.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T12:55:32.798518Z",
     "start_time": "2020-05-29T12:55:32.792555Z"
    }
   },
   "outputs": [],
   "source": [
    "a.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T12:58:24.560103Z",
     "start_time": "2020-05-29T12:58:23.758560Z"
    }
   },
   "outputs": [],
   "source": [
    "net = DenseNet121(out_size=3, mode='U-Ones', drop_rate=0.2)\n",
    "net = torch.nn.DataParallel(net)\n",
    "net = net.cuda()\n",
    "checkpoint = torch.load(\"../model/0528_hip_test_label_rate09/checkpoint/epoch_12.pth\")\n",
    "net.load_state_dict(checkpoint['state_dict'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T13:23:10.901221Z",
     "start_time": "2020-05-29T13:23:01.367348Z"
    }
   },
   "outputs": [],
   "source": [
    "net.eval()\n",
    "count = 0\n",
    "total = 0\n",
    "for i, (study, _, image, label) in enumerate(test_dataloader):\n",
    "    #print(\"image:\",study)\n",
    "    image,label = image.cuda(),label.cuda()\n",
    "    gt_label = torch.max(label, 1)[1]\n",
    "    if gt_label == 1:\n",
    "        total +=1\n",
    "        print(\"gt label:\",gt_label)\n",
    "        _,output = net(image)\n",
    "        pred_label = torch.max(output, 1)[1]\n",
    "        if pred_label == gt_label:\n",
    "            count+=1\n",
    "        print(\"pred_label:\",pred_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T13:21:13.962734Z",
     "start_time": "2020-05-29T13:21:13.954344Z"
    }
   },
   "outputs": [],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T13:17:04.241069Z",
     "start_time": "2020-05-29T13:17:04.232671Z"
    }
   },
   "outputs": [],
   "source": [
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T13:21:26.269959Z",
     "start_time": "2020-05-29T13:21:26.264360Z"
    }
   },
   "outputs": [],
   "source": [
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T05:05:13.592348Z",
     "start_time": "2020-05-31T05:05:13.541297Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "df = pd.read_csv(\"../dataset/hip_4cls/training.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T05:03:50.615647Z",
     "start_time": "2020-05-31T05:03:50.604461Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df.sample(frac = 0.025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T05:03:54.775963Z",
     "start_time": "2020-05-31T05:03:54.768960Z"
    }
   },
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T05:04:09.072557Z",
     "start_time": "2020-05-31T05:04:09.057197Z"
    }
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"../dataset/hip_4cls/training_frac002.csv\",index=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get  traing data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-30T05:22:55.010559Z",
     "start_time": "2020-05-30T05:22:55.005900Z"
    }
   },
   "outputs": [],
   "source": [
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T02:33:58.197584Z",
     "start_time": "2020-05-31T02:33:57.276866Z"
    }
   },
   "outputs": [],
   "source": [
    "data_list = glob(\"../../hipX_largedata/data/512_2-1_train_3cls_new/*/*/*png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T02:34:11.629166Z",
     "start_time": "2020-05-31T02:34:11.622515Z"
    }
   },
   "outputs": [],
   "source": [
    "len(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T02:35:41.737135Z",
     "start_time": "2020-05-31T02:35:18.657998Z"
    }
   },
   "outputs": [],
   "source": [
    "# for semi-supervised\n",
    "df =pd.DataFrame(columns=('image','disease'))\n",
    "for img in data_list:\n",
    "    img_name = img.split(\"/\")[-1]\n",
    "    label = img.split(\"/\")[-2][0]\n",
    "    df=df.append(pd.DataFrame({'image':[img_name],'disease':[label]}),ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T02:36:45.625264Z",
     "start_time": "2020-05-31T02:36:45.551908Z"
    }
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"../../semi_supervised_cls/dataset/hip_4cls/training2.csv\",index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T02:40:29.851680Z",
     "start_time": "2020-05-31T02:40:29.801900Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../../semi_supervised_cls/dataset/hip_4cls/training2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T02:40:41.547137Z",
     "start_time": "2020-05-31T02:40:41.529568Z"
    }
   },
   "outputs": [],
   "source": [
    "col = 'disease'\n",
    "data = df\n",
    "data[col] = data[col].astype('category')#转换成数据类别类型，pandas用法\n",
    "dummy = pd.get_dummies(data[col])  #get_dummies为pandas里面求哑变量的包\n",
    "#dummy = dummy.add_prefix('{}#'.format(col)) #add_prefix为加上前缀\n",
    "#data.drop(col,axis = 1,inplace = True)\n",
    "data = data.join(dummy) #index即为userid，所以可以用join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T02:42:18.945256Z",
     "start_time": "2020-05-31T02:42:18.938157Z"
    }
   },
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T02:41:50.311795Z",
     "start_time": "2020-05-31T02:41:50.297741Z"
    }
   },
   "outputs": [],
   "source": [
    "order = ['image',0,1,2,3,'disease']\n",
    "data = data[order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T02:44:29.610708Z",
     "start_time": "2020-05-31T02:44:29.342159Z"
    }
   },
   "outputs": [],
   "source": [
    "df = data.sample(frac=1.0).reset_index(drop=True)\n",
    "df[0:19032].to_csv(\"../dataset/hip_4cls/training.csv\",index=0)\n",
    "df[19032:21750].to_csv(\"../dataset/hip_4cls/validation.csv\",index=0)\n",
    "df[21750:].to_csv(\"../dataset/hip_4cls/testing.csv\",index=0)\n",
    "#df_train = data.sample(frac=0.7).reset_index(drop=True)\n",
    "#df_train = data.sample(frac=0.7).reset_index(drop=True)\n",
    "#df_train = data.sample(frac=0.7).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"../../semi_supervised_cls/dataset/hip/testing_fold2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get diabetics data training file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T14:15:26.973918Z",
     "start_time": "2020-06-01T14:15:26.933844Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../dataset/diabetics/trainLabels_cropped.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T14:15:34.349212Z",
     "start_time": "2020-06-01T14:15:34.333577Z"
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T14:16:01.518818Z",
     "start_time": "2020-06-01T14:16:01.498911Z"
    }
   },
   "outputs": [],
   "source": [
    "col = 'level'\n",
    "data = df\n",
    "data[col] = data[col].astype('category')#转换成数据类别类型，pandas用法\n",
    "dummy = pd.get_dummies(data[col])  #get_dummies为pandas里面求哑变量的包\n",
    "#dummy = dummy.add_prefix('{}#'.format(col)) #add_prefix为加上前缀\n",
    "#data.drop(col,axis = 1,inplace = True)\n",
    "data = data.join(dummy) #index即为userid，所以可以用join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T14:16:09.836706Z",
     "start_time": "2020-06-01T14:16:09.817465Z"
    }
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T14:17:26.771952Z",
     "start_time": "2020-06-01T14:17:26.746184Z"
    }
   },
   "outputs": [],
   "source": [
    "data['image'] = data['image'].apply(lambda x: x+\".jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T15:43:06.401971Z",
     "start_time": "2020-06-01T15:43:06.394495Z"
    }
   },
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T14:18:41.986396Z",
     "start_time": "2020-06-01T14:18:41.974336Z"
    }
   },
   "outputs": [],
   "source": [
    "order = ['image',0,1,2,3,4,'level']\n",
    "data = data[order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T15:44:14.968896Z",
     "start_time": "2020-06-01T15:44:14.699072Z"
    }
   },
   "outputs": [],
   "source": [
    "df = data.sample(frac=1.0).reset_index(drop=True)\n",
    "df[0:24576].to_csv(\"../dataset/diabetics/training.csv\",index=0)\n",
    "df[24576:28086].to_csv(\"../dataset/diabetics/validation.csv\",index=0)\n",
    "df[28086:].to_csv(\"../dataset/diabetics/testing.csv\",index=0)\n",
    "#df_train = data.sample(frac=0.7).reset_index(drop=True)\n",
    "#df_train = data.sample(frac=0.7).reset_index(drop=True)\n",
    "#df_train = data.sample(frac=0.7).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test bnm loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T09:16:34.131571Z",
     "start_time": "2020-06-02T09:16:34.123638Z"
    }
   },
   "outputs": [],
   "source": [
    "A = torch.FloatTensor([[0.6,0.2,0.2],[0.3,0.3,0.4],[1,0,0],[0.4,0.4,0.2]]) +0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T09:16:36.838305Z",
     "start_time": "2020-06-02T09:16:36.833826Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "B = -1.0 * A*torch.log(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T09:16:39.124415Z",
     "start_time": "2020-06-02T09:16:39.117027Z"
    }
   },
   "outputs": [],
   "source": [
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T09:16:42.177103Z",
     "start_time": "2020-06-02T09:16:42.171615Z"
    }
   },
   "outputs": [],
   "source": [
    "C = B.sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T09:16:45.185982Z",
     "start_time": "2020-06-02T09:16:45.178476Z"
    }
   },
   "outputs": [],
   "source": [
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T09:16:49.615778Z",
     "start_time": "2020-06-02T09:16:49.611305Z"
    }
   },
   "outputs": [],
   "source": [
    "index = C.argsort(descending=True)[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T09:16:52.652484Z",
     "start_time": "2020-06-02T09:16:52.646651Z"
    }
   },
   "outputs": [],
   "source": [
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T09:17:15.704759Z",
     "start_time": "2020-06-02T09:17:15.694709Z"
    }
   },
   "outputs": [],
   "source": [
    "A[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sample from every class independent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-10T01:20:54.390886Z",
     "start_time": "2020-06-10T01:20:54.342334Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "df = pd.read_csv('../dataset/hip_4cls/training_sample_single_class.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-10T01:26:56.952497Z",
     "start_time": "2020-06-10T01:26:56.939167Z"
    }
   },
   "outputs": [],
   "source": [
    "df_0 = df[0:4218]\n",
    "df_1 = df[4218:8266]\n",
    "df_2 = df[8266:15149]\n",
    "df_3 = df[15149:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-10T01:32:36.734192Z",
     "start_time": "2020-06-10T01:32:36.714379Z"
    }
   },
   "outputs": [],
   "source": [
    "df_0 = df_0.sample(frac=1.0).reset_index(drop=True)\n",
    "df_1 = df_1.sample(frac=1.0).reset_index(drop=True)\n",
    "df_2 = df_2.sample(frac=1.0).reset_index(drop=True)\n",
    "df_3 = df_3.sample(frac=1.0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-10T01:32:41.448714Z",
     "start_time": "2020-06-10T01:32:41.439958Z"
    }
   },
   "outputs": [],
   "source": [
    "df_new = pd.concat([df_0[0:len(df_0)*0.1*x],df_1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-10T01:32:42.014337Z",
     "start_time": "2020-06-10T01:32:42.001121Z"
    }
   },
   "outputs": [],
   "source": [
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-10T02:11:44.053961Z",
     "start_time": "2020-06-10T02:11:43.815414Z"
    }
   },
   "outputs": [],
   "source": [
    "df_new = pd.DataFrame(columns=['image','0','1','2','3','disease'])\n",
    "for i in range(0,50):\n",
    "    left = int()\n",
    "    r = 0.02\n",
    "    df_cat = pd.concat([df_0[int(len(df_0)*r*i):int(len(df_0)*r*(i+1))],\n",
    "                    df_1[int(len(df_1)*r*i):int(len(df_1)*r*(i+1))],\n",
    "                   df_2[int(len(df_2)*r*i):int(len(df_2)*r*(i+1))],\n",
    "                   df_3[int(len(df_3)*r*i):int(len(df_3)*r*(i+1))]])\n",
    "    df_cat = df_cat.sample(frac=1.0).reset_index(drop=True)\n",
    "    df_new = pd.concat([df_new,df_cat])\n",
    "    #df_new = df_new.sample(frac=1.0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-10T02:13:14.946854Z",
     "start_time": "2020-06-10T02:13:14.925108Z"
    }
   },
   "outputs": [],
   "source": [
    "df_new.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-10T02:16:34.370903Z",
     "start_time": "2020-06-10T02:16:34.276955Z"
    }
   },
   "outputs": [],
   "source": [
    "df_new.to_csv(\"../dataset/hip_4cls/training_sample_single_class.csv\",index=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-scale densenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T07:49:29.881478Z",
     "start_time": "2020-07-06T07:49:29.835202Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from networks import densenet\n",
    "from collections import OrderedDict\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "\n",
    "\n",
    "class _DenseLayer(nn.Sequential):\n",
    "    def __init__(self, num_input_features, growth_rate, bn_size, drop_rate):\n",
    "        super(_DenseLayer, self).__init__()\n",
    "        self.add_module('norm1', nn.BatchNorm2d(num_input_features)),\n",
    "        self.add_module('relu1', nn.ReLU(inplace=True)),\n",
    "        self.add_module('conv1', nn.Conv2d(num_input_features, bn_size *\n",
    "                        growth_rate, kernel_size=1, stride=1, bias=False)),\n",
    "        self.add_module('norm2', nn.BatchNorm2d(bn_size * growth_rate)),\n",
    "        self.add_module('relu2', nn.ReLU(inplace=True)),\n",
    "        self.add_module('conv2', nn.Conv2d(bn_size * growth_rate, growth_rate,\n",
    "                        kernel_size=3, stride=1, padding=1, bias=False)),\n",
    "        self.drop_rate = drop_rate\n",
    "        self.drop_layer = nn.Dropout(p=drop_rate)\n",
    "    def forward(self, x):\n",
    "        new_features = super(_DenseLayer, self).forward(x)\n",
    "        # if self.drop_rate > 0:\n",
    "        #     print (self.drop_rate)\n",
    "        #     new_features = self.drop_layer(new_features)\n",
    "        return torch.cat([x, new_features], 1)\n",
    "\n",
    "\n",
    "class _DenseBlock(nn.Sequential):\n",
    "    def __init__(self, num_layers, num_input_features, bn_size, growth_rate, drop_rate):\n",
    "        super(_DenseBlock, self).__init__()\n",
    "        for i in range(num_layers):\n",
    "            layer = _DenseLayer(num_input_features + i * growth_rate, growth_rate, bn_size, drop_rate)\n",
    "            self.add_module('denselayer%d' % (i + 1), layer)\n",
    "\n",
    "\n",
    "class _Transition(nn.Sequential):\n",
    "    def __init__(self, num_input_features, num_output_features):\n",
    "        super(_Transition, self).__init__()\n",
    "        self.add_module('norm', nn.BatchNorm2d(num_input_features))\n",
    "        self.add_module('relu', nn.ReLU(inplace=True))\n",
    "        self.add_module('conv', nn.Conv2d(num_input_features, num_output_features,\n",
    "                                          kernel_size=1, stride=1, bias=False))\n",
    "        self.add_module('pool', nn.AvgPool2d(kernel_size=2, stride=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T07:49:34.323533Z",
     "start_time": "2020-07-06T07:49:34.211252Z"
    }
   },
   "outputs": [],
   "source": [
    "class DenseNetMultiScale(nn.Module):\n",
    "    r\"\"\"Densenet-BC model class, based on\n",
    "    `\"Densely Connected Convolutional Networks\" <https://arxiv.org/pdf/1608.06993.pdf>`_\n",
    "    Args:\n",
    "        growth_rate (int) - how many filters to add each layer (`k` in paper)\n",
    "        block_config (list of 4 ints) - how many layers in each pooling block\n",
    "        num_init_features (int) - the number of filters to learn in the first convolution layer\n",
    "        bn_size (int) - multiplicative factor for number of bottle neck layers\n",
    "          (i.e. bn_size * k features in the bottleneck layer)\n",
    "        drop_rate (float) - dropout rate after each dense layer\n",
    "        num_classes (int) - number of classification classes\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, growth_rate=32, block_config=(6, 12, 24, 16),\n",
    "                 num_init_features=64, bn_size=4, drop_rate=0, num_classes=1000):\n",
    "\n",
    "        super(DenseNetMultiScale, self).__init__()\n",
    "\n",
    "        # First convolution\n",
    "        self.features = nn.Sequential(OrderedDict([\n",
    "            ('conv0', nn.Conv2d(3, num_init_features, kernel_size=7, stride=2, padding=3, bias=False)),\n",
    "            ('norm0', nn.BatchNorm2d(num_init_features)),\n",
    "            ('relu0', nn.ReLU(inplace=True)),\n",
    "            ('pool0', nn.MaxPool2d(kernel_size=3, stride=2, padding=1)),\n",
    "        ]))\n",
    "\n",
    "        # Each denseblock\n",
    "        num_features = num_init_features\n",
    "        self.denseblock_list = []\n",
    "        for i, num_layers in enumerate(block_config):\n",
    "            block = _DenseBlock(num_layers=num_layers, num_input_features=num_features,\n",
    "                                bn_size=bn_size, growth_rate=growth_rate, drop_rate=drop_rate)\n",
    "            self.features.add_module('denseblock%d' % (i + 1), block)\n",
    "            num_features = num_features + num_layers * growth_rate\n",
    "            if i != len(block_config) - 1:\n",
    "                trans = _Transition(num_input_features=num_features, num_output_features=num_features // 2)\n",
    "                self.features.add_module('transition%d' % (i + 1), trans)\n",
    "                num_features = num_features // 2\n",
    "            if i != len(block_config) - 1:\n",
    "                self.denseblock_list.append(nn.Sequential(block,trans))\n",
    "            else:\n",
    "                self.denseblock_list.append(nn.Sequential(block))\n",
    "#         self.denseblock1 = self.denseblock_list[0]\n",
    "#         self.denseblock2 = self.denseblock_list[1]\n",
    "#         self.denseblock3 = self.denseblock_list[2]\n",
    "#         self.denseblock4 = self.denseblock_list[3]\n",
    "        # Final batch norm\n",
    "        self.features.add_module('norm5', nn.BatchNorm2d(num_features))\n",
    "        # Linear layer\n",
    "        self.classifier = nn.Linear(num_features, num_classes)\n",
    "\n",
    "        # Official init from torch repo.\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features.conv0(x)\n",
    "        x = self.features.norm0(x)\n",
    "        x = self.features.relu0(x)\n",
    "        x = self.features.pool0(x)\n",
    "        x = self.features.denseblock1(x)\n",
    "        fea1 = self.features.transition1(x)\n",
    "        fea2 = self.features.denseblock2(fea1)\n",
    "        fea2 = self.features.transition2(fea2)\n",
    "        fea3 = self.features.denseblock3(fea2)\n",
    "        fea3 = self.features.transition3(fea3)\n",
    "        features = self.features.denseblock4(fea3)\n",
    "        print(\"fea1 shape:\",fea1.shape)\n",
    "        print(\"fea2 shape:\",fea2.shape)\n",
    "        print(\"fea3 shape:\",fea3.shape)\n",
    "        out = F.relu(features, inplace=True)\n",
    "        fea_out2 = F.adaptive_avg_pool2d(fea2, (1, 1)).view(fea2.size(0), -1)\n",
    "        fea_out3 = F.adaptive_avg_pool2d(fea3, (1, 1)).view(fea3.size(0), -1)\n",
    "        fea_out = F.adaptive_avg_pool2d(out, (1, 1)).view(features.size(0), -1)\n",
    "        print(fea_out.size())\n",
    "        out = self.classifier(fea_out)\n",
    "        return [fea_out2,fea_out3,fea_out],out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T07:55:37.946252Z",
     "start_time": "2020-07-06T07:55:37.895884Z"
    }
   },
   "outputs": [],
   "source": [
    "class DenseNet121MultiScale(nn.Module):\n",
    "    \"\"\"Model modified.\n",
    "    The architecture of our model is the same as standard DenseNet121\n",
    "    except the classifier layer which has an additional sigmoid function.\n",
    "    \"\"\"\n",
    "    def __init__(self, out_size,drop_rate=0):\n",
    "        super(DenseNet121MultiScale, self).__init__()\n",
    "        self.densenet121 = densenet.densenet121(pretrained=True, drop_rate=drop_rate)\n",
    "        num_ftrs = self.densenet121.classifier.in_features\n",
    "        self.densenet121.classifier = nn.Sequential(\n",
    "            nn.Linear(num_ftrs, out_size),\n",
    "            #nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        # Official init from torch repo.\n",
    "        for m in self.densenet121.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        self.drop_rate = drop_rate\n",
    "        self.drop_layer = nn.Dropout(p=drop_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.densenet121.features.conv0(x)\n",
    "        x = self.densenet121.features.norm0(x)\n",
    "        x = self.densenet121.features.relu0(x)\n",
    "        x = self.densenet121.features.pool0(x)\n",
    "        x = self.densenet121.features.denseblock1(x)\n",
    "        fea1 = self.densenet121.features.transition1(x)\n",
    "        fea2 = self.densenet121.features.denseblock2(fea1)\n",
    "        fea2 = self.densenet121.features.transition2(fea2)\n",
    "        fea3 = self.densenet121.features.denseblock3(fea2)\n",
    "        fea3 = self.densenet121.features.transition3(fea3)\n",
    "        print(\"fea1 shape:\",fea1.shape)\n",
    "        print(\"fea2 shape:\",fea2.shape)\n",
    "        print(\"fea3 shape:\",fea3.shape)\n",
    "        features = self.densenet121.features.denseblock4(fea3)\n",
    "        features = self.densenet121.features.norm5(features)\n",
    "        out = F.relu(features, inplace=True) \n",
    "        fea_out2 = F.adaptive_avg_pool2d(fea2, (1, 1)).view(fea2.size(0), -1)\n",
    "        fea_out3 = F.adaptive_avg_pool2d(fea3, (1, 1)).view(fea3.size(0), -1)\n",
    "        out = F.adaptive_avg_pool2d(out, (1, 1)).view(features.size(0), -1)\n",
    "        if self.drop_rate > 0:\n",
    "            out = self.drop_layer(out)\n",
    "        self.activations = out\n",
    "        out = self.densenet121.classifier(out)\n",
    "            \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T07:55:58.255132Z",
     "start_time": "2020-07-06T07:55:58.215772Z"
    }
   },
   "outputs": [],
   "source": [
    "class DenseNet121(nn.Module):\n",
    "    \"\"\"Model modified.\n",
    "    The architecture of our model is the same as standard DenseNet121\n",
    "    except the classifier layer which has an additional sigmoid function.\n",
    "    \"\"\"\n",
    "    def __init__(self, out_size, mode, drop_rate=0):\n",
    "        super(DenseNet121, self).__init__()\n",
    "        assert mode in ('U-Ones', 'U-Zeros', 'U-MultiClass')\n",
    "        self.densenet121 = densenet.densenet121(pretrained=True, drop_rate=drop_rate)\n",
    "        num_ftrs = self.densenet121.classifier.in_features\n",
    "        if mode in ('U-Ones', 'U-Zeros'):\n",
    "            self.densenet121.classifier = nn.Sequential(\n",
    "                nn.Linear(num_ftrs, out_size),\n",
    "                #nn.Sigmoid()\n",
    "            )\n",
    "        elif mode in ('U-MultiClass', ):\n",
    "            self.densenet121.classifier = None\n",
    "            self.densenet121.Linear_0 = nn.Linear(num_ftrs, out_size)\n",
    "            self.densenet121.Linear_1 = nn.Linear(num_ftrs, out_size)\n",
    "            self.densenet121.Linear_u = nn.Linear(num_ftrs, out_size)\n",
    "            \n",
    "        self.mode = mode\n",
    "        \n",
    "        # Official init from torch repo.\n",
    "        for m in self.densenet121.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        self.drop_rate = drop_rate\n",
    "        self.drop_layer = nn.Dropout(p=drop_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        ori = x\n",
    "        features = self.densenet121.features(x)\n",
    "        out = F.relu(features, inplace=True)\n",
    "        \n",
    "        \n",
    "        out = F.adaptive_avg_pool2d(out, (1, 1)).view(features.size(0), -1)\n",
    "\n",
    "        if self.drop_rate > 0:\n",
    "            out = self.drop_layer(out)\n",
    "        self.activations = out\n",
    "        out = self.densenet121.classifier(out)\n",
    "            \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-13T07:25:53.833231Z",
     "start_time": "2020-06-13T07:25:53.104586Z"
    }
   },
   "outputs": [],
   "source": [
    "#model = DenseNetMultiScale(num_init_features=64, growth_rate=32, block_config=(6, 12, 24, 16))\n",
    "model1 = DenseNet121MultiScale(out_size=4, drop_rate=0)\n",
    "model2 = DenseNet121(out_size=4, mode='U-Ones', drop_rate=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-13T07:26:10.530904Z",
     "start_time": "2020-06-13T07:26:09.407001Z"
    }
   },
   "outputs": [],
   "source": [
    "image = torch.rand(3,3,256,256)\n",
    "ori1,fea1,out1 = model1(image)\n",
    "ori2,fea2,out2 = model2(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-14T08:11:49.869620Z",
     "start_time": "2020-06-14T08:11:49.856437Z"
    }
   },
   "outputs": [],
   "source": [
    "fea1[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T06:15:03.508866Z",
     "start_time": "2020-06-12T06:15:03.482002Z"
    }
   },
   "outputs": [],
   "source": [
    "ori2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-11T14:18:30.276543Z",
     "start_time": "2020-06-11T14:18:30.106922Z"
    }
   },
   "outputs": [],
   "source": [
    "model_urls = {\n",
    "    'densenet121': 'https://download.pytorch.org/models/densenet121-a639ec97.pth',\n",
    "    'densenet169': 'https://download.pytorch.org/models/densenet169-b2777c0a.pth',\n",
    "    'densenet201': 'https://download.pytorch.org/models/densenet201-c1103571.pth',\n",
    "    'densenet161': 'https://download.pytorch.org/models/densenet161-8d451a50.pth',\n",
    "}\n",
    "pattern = re.compile(\n",
    "    r'^(.*denselayer\\d+\\.(?:norm|relu|conv))\\.((?:[12])\\.(?:weight|bias|running_mean|running_var))$')\n",
    "state_dict = model_zoo.load_url(model_urls['densenet121'])\n",
    "for key in list(state_dict.keys()):\n",
    "    res = pattern.match(key)\n",
    "    if res:\n",
    "        new_key = res.group(1) + res.group(2)\n",
    "        state_dict[new_key] = state_dict[key]\n",
    "        del state_dict[key]\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-11T12:16:15.588873Z",
     "start_time": "2020-06-11T12:16:14.826419Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-11T12:16:29.804317Z",
     "start_time": "2020-06-11T12:16:29.795135Z"
    }
   },
   "outputs": [],
   "source": [
    "fea[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test BNM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T08:14:36.032835Z",
     "start_time": "2020-06-16T08:14:36.014145Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "# from tensorboardX import SummaryWriter\n",
    "import shutil\n",
    "import argparse\n",
    "import logging\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import make_grid\n",
    "import pretrainedmodels\n",
    "\n",
    "from networks.models import DenseNet121,DenseNet161\n",
    "from utils import losses, ramps\n",
    "from utils.metrics import compute_AUCs\n",
    "from utils.metric_logger import MetricLogger\n",
    "from dataloaders import  dataset\n",
    "from dataloaders import chest_xray_14\n",
    "from dataloaders.dataset import TwoStreamBatchSampler\n",
    "from utils.util import get_timestamp\n",
    "from validation import epochVal, epochVal_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T08:15:08.628890Z",
     "start_time": "2020-06-16T08:15:08.609069Z"
    }
   },
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                     [0.229, 0.224, 0.225])\n",
    "resize = 224\n",
    "test_dataset = dataset.CheXpertDataset(root_dir='../dataset/skin/training_data/',\n",
    "                                        csv_file='../dataset/skin/testing.csv',\n",
    "                                        transform=transforms.Compose([\n",
    "                                              transforms.Resize((resize, resize)),\n",
    "                                              transforms.ToTensor(),\n",
    "                                              normalize,\n",
    "                                        ]))\n",
    "print(\"train_dataset len:\",len(test_dataset))\n",
    "test_dataloader = DataLoader(dataset=test_dataset, batch_size=1,\n",
    "                                shuffle=False, num_workers=8, pin_memory=True)#, worker_init_fn=worker_init_fn)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-18T04:24:24.016716Z",
     "start_time": "2020-06-18T04:24:22.451456Z"
    }
   },
   "outputs": [],
   "source": [
    "model_baseline = DenseNet121(out_size=7, mode='U-Ones', drop_rate=0.2)\n",
    "model_bnm = DenseNet121(out_size=7, mode='U-Ones', drop_rate=0.2)\n",
    "model_baseline = torch.nn.DataParallel(model_baseline).cuda()\n",
    "model_bnm = torch.nn.DataParallel(model_bnm).cuda()\n",
    "#checkpoint_baseline = torch.load(\"../model/skin_frac01_baseline/epoch_92.pth\")\n",
    "checkpoint_baseline = torch.load(\"../model/skin_frac01_mt/epoch_31.pth\")\n",
    "checkpoint_bnm = torch.load(\"../model/skin_frac01_mt_bnm/epoch_68.pth\")\n",
    "model_baseline.load_state_dict(checkpoint_baseline['state_dict'])\n",
    "model_bnm.load_state_dict(checkpoint_bnm['state_dict'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-18T04:29:43.629224Z",
     "start_time": "2020-06-18T04:24:35.626851Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_baseline.eval()\n",
    "model_bnm.eval()\n",
    "cnt_total = 0\n",
    "cnt_06 = 0\n",
    "cnt_07 = 0\n",
    "total = [0,0,0,0,0,0,0]\n",
    "correct_baseline = [0,0,0,0,0,0,0]\n",
    "correct_bnm = [0,0,0,0,0,0,0]\n",
    "pred_baseline = [0,0,0,0,0,0,0]\n",
    "pre_bnm = [0 for i in range(7)]\n",
    "gt_study   = {}\n",
    "pred_study = {}\n",
    "studies    = []\n",
    "prob_list = [0.5,0.6,0.7,0.8,0.9,0.95,0.97,0.98,0.99]\n",
    "cnt_prob_total = [0 for i in range(9)]\n",
    "cnt_prob_correct = [0 for i in range(9)]\n",
    "print(\"eval\")\n",
    "with torch.no_grad():\n",
    "    for i,(study, _, image, label) in enumerate(test_dataloader):\n",
    "        image, label = image.cuda(), label.cuda()\n",
    "        _,output_baseline = model_baseline(image)\n",
    "        _,output_bnm = model_bnm(image)\n",
    "        output_baseline = F.softmax(output_baseline,dim=1)\n",
    "        output_bnm = F.softmax(output_bnm,dim=1)\n",
    "        label = torch.max(label, 1)[1]\n",
    "        label_baseline = torch.max(output_baseline, 1)[1]\n",
    "        label_bnm = torch.max(output_bnm, 1)[1]\n",
    "        pred_baseline[label_baseline]+=1\n",
    "        pre_bnm[label_bnm]+=1\n",
    "        total[label]+=1\n",
    "        if label == label_bnm:\n",
    "            correct_bnm[label]+= 1\n",
    "        if label == label_baseline:\n",
    "            correct_baseline[label] += 1\n",
    "\n",
    "        output_baseline_np = output_baseline.cpu().detach().numpy()\n",
    "        output_bnm_np = output_bnm.cpu().detach().numpy()\n",
    "        if label_baseline == 1:\n",
    "            cnt_total += 1\n",
    "            for j,prob in enumerate(prob_list):\n",
    "                if np.max(output_baseline_np)<=prob:\n",
    "                    #print(\"label:\",label)\n",
    "                    cnt_prob_total[j]+=1\n",
    "#                     print(\"baseline pred:\",output_baseline_np)\n",
    "#                     print(\"bnm pred:\",output_bnm_np)\n",
    "                    if label_baseline != label_bnm:\n",
    "                        cnt_prob_correct[j]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-18T04:49:21.942791Z",
     "start_time": "2020-06-18T04:49:21.931119Z"
    }
   },
   "outputs": [],
   "source": [
    "cnt_prob_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-18T04:49:31.578533Z",
     "start_time": "2020-06-18T04:49:31.569913Z"
    }
   },
   "outputs": [],
   "source": [
    "#0.5,0.6,0.7,0.8,0.9,0.95,0.97,0.98,0.99\n",
    "cnt_prob_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-18T04:50:15.627206Z",
     "start_time": "2020-06-18T04:50:15.618879Z"
    }
   },
   "outputs": [],
   "source": [
    "pred_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-18T04:50:21.325097Z",
     "start_time": "2020-06-18T04:50:21.319358Z"
    }
   },
   "outputs": [],
   "source": [
    "pre_bnm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T13:29:03.458005Z",
     "start_time": "2020-06-16T13:29:03.453201Z"
    }
   },
   "outputs": [],
   "source": [
    "cnt_06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-18T04:10:21.702033Z",
     "start_time": "2020-06-18T04:10:21.693769Z"
    }
   },
   "outputs": [],
   "source": [
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-18T05:17:48.790841Z",
     "start_time": "2020-06-18T05:17:48.782672Z"
    }
   },
   "outputs": [],
   "source": [
    "correct_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-18T05:17:54.981572Z",
     "start_time": "2020-06-18T05:17:54.974032Z"
    }
   },
   "outputs": [],
   "source": [
    "correct_bnm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T08:34:20.918982Z",
     "start_time": "2020-06-16T08:34:20.910154Z"
    }
   },
   "outputs": [],
   "source": [
    "acc = 0.0\n",
    "for i in range(len(total)):\n",
    "    acc+= correct_baseline[i]/total[i]\n",
    "    print(correct_baseline[i]/total[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T08:33:52.600418Z",
     "start_time": "2020-06-16T08:33:52.595330Z"
    }
   },
   "outputs": [],
   "source": [
    "acc = 0.0\n",
    "for i in range(len(total)):\n",
    "    acc +=correct_bnm[i]/total[i]\n",
    "    print(correct_bnm[i]/total[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T07:43:49.747409Z",
     "start_time": "2020-06-16T07:43:49.738725Z"
    }
   },
   "outputs": [],
   "source": [
    "acc/7.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T07:44:14.530221Z",
     "start_time": "2020-06-16T07:44:14.522556Z"
    }
   },
   "outputs": [],
   "source": [
    "acc/7.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# wcp loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T07:03:59.304492Z",
     "start_time": "2020-07-06T07:03:57.844698Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import chainer\n",
    "import chainer.functions as F_chainer\n",
    "from chainer import Variable as V_chain\n",
    "import cupy as xp\n",
    "from cupy.core.dlpack import toDlpack\n",
    "from cupy.core.dlpack import fromDlpack\n",
    "from torch.utils.dlpack import to_dlpack\n",
    "from torch.utils.dlpack import from_dlpack\n",
    "from source.chainer_functions.loss import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T07:04:00.271622Z",
     "start_time": "2020-07-06T07:04:00.237517Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from networks import densenet\n",
    "from collections import OrderedDict\n",
    "import torch.utils.model_zoo as model_zoo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## densenet121 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-05T07:44:25.248663Z",
     "start_time": "2020-07-05T07:44:25.198789Z"
    }
   },
   "outputs": [],
   "source": [
    "class DenseNet121MultiScale(nn.Module):\n",
    "    \"\"\"Model modified.\n",
    "    The architecture of our model is the same as standard DenseNet121\n",
    "    except the classifier layer which has an additional sigmoid function.\n",
    "    \"\"\"\n",
    "    def __init__(self, out_size,drop_rate=0):\n",
    "        super(DenseNet121MultiScale, self).__init__()\n",
    "        self.densenet121 = densenet.densenet121(pretrained=True, drop_rate=drop_rate)\n",
    "        num_ftrs = self.densenet121.classifier.in_features\n",
    "        self.densenet121.classifier = nn.Sequential(\n",
    "            nn.Linear(num_ftrs, out_size),\n",
    "            #nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        # Official init from torch repo.\n",
    "        for m in self.densenet121.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        self.drop_rate = drop_rate\n",
    "        self.drop_layer = nn.Dropout(p=drop_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.densenet121.features.conv0(x)\n",
    "        x = self.densenet121.features.norm0(x)\n",
    "        x = self.densenet121.features.relu0(x)\n",
    "        x = self.densenet121.features.pool0(x)\n",
    "        x = self.densenet121.features.denseblock1(x)\n",
    "        fea1 = self.densenet121.features.transition1(x)\n",
    "        fea2 = self.densenet121.features.denseblock2(fea1)\n",
    "        fea2 = self.densenet121.features.transition2(fea2)\n",
    "        fea3 = self.densenet121.features.denseblock3(fea2)\n",
    "        fea3 = self.densenet121.features.transition3(fea3)\n",
    "        print(\"fea1 shape:\",fea1.shape)\n",
    "        print(\"fea2 shape:\",fea2.shape)\n",
    "        print(\"fea3 shape:\",fea3.shape)\n",
    "        features = self.densenet121.features.denseblock4(fea3)\n",
    "        features = self.densenet121.features.norm5(features)\n",
    "        out = F.relu(features, inplace=True) \n",
    "        fea_out2 = F.adaptive_avg_pool2d(fea2, (1, 1)).view(fea2.size(0), -1)\n",
    "        fea_out3 = F.adaptive_avg_pool2d(fea3, (1, 1)).view(fea3.size(0), -1)\n",
    "        out = F.adaptive_avg_pool2d(out, (1, 1)).view(features.size(0), -1)\n",
    "        if self.drop_rate > 0:\n",
    "            out = self.drop_layer(out)\n",
    "        self.activations = out\n",
    "        out = self.densenet121.classifier(out)\n",
    "            \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## wcp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T07:56:35.652427Z",
     "start_time": "2020-07-06T07:56:09.813054Z"
    }
   },
   "outputs": [],
   "source": [
    "model = DenseNet121(out_size=4, mode='U-Ones', drop_rate=0)\n",
    "image = torch.rand(3,3,224,224)\n",
    "out = model(image)\n",
    "loos = wcp_loss_torch(model,image,out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T07:56:41.702669Z",
     "start_time": "2020-07-06T07:56:41.681444Z"
    }
   },
   "outputs": [],
   "source": [
    "loos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-05T09:08:40.415816Z",
     "start_time": "2020-07-05T09:08:37.964218Z"
    }
   },
   "outputs": [],
   "source": [
    "_,_,output = wcp_loss_torch(model,image,out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T07:51:27.545949Z",
     "start_time": "2020-07-06T07:51:27.406781Z"
    }
   },
   "outputs": [],
   "source": [
    "for name in model.state_dict():\n",
    "#     if \"bias\" in name or \"norm\" in name or \"classifier\" in name:\n",
    "#         continue\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-05T08:50:27.505046Z",
     "start_time": "2020-07-05T08:50:27.482591Z"
    }
   },
   "outputs": [],
   "source": [
    "model.state_dict()['features.denseblock1.denselayer1.norm1.num_batches_tracked']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T07:05:09.366156Z",
     "start_time": "2020-07-06T07:05:09.185096Z"
    }
   },
   "outputs": [],
   "source": [
    "def wcp_loss_torch(model, x, logit, epsilon=8., dr=0.5, num_simulations=1, xi=1e-6):\n",
    "    n_batch = x.shape[0]\n",
    "    size_list = [0]\n",
    "    size_sum = 0\n",
    "    for name in model.state_dict():\n",
    "        if \"bias\" in name or \"norm\" in name or \"classifier\" in name:\n",
    "            continue\n",
    "        size = model.state_dict()[name].numel()\n",
    "        size_sum += size \n",
    "        size_list += [size_sum]\n",
    "    d = xp.random.normal(size=x.shape)\n",
    "    d /= (1e-12 + xp.max(xp.abs(d),range(1, len(d.shape)), keepdims=True))\n",
    "    d /= xp.sqrt(1e-6 + xp.sum(d ** 2, range(1, len(d.shape)), keepdims=True))\n",
    "    d = from_dlpack(toDlpack(d)).type(torch.FloatTensor)\n",
    "    \n",
    "    d_weight = xp.random.normal(size=size_list[-1])\n",
    "    d_weight /= (1e-12 + xp.max(xp.abs(d_weight)))\n",
    "    d_weight /= xp.sqrt(1e-6 + xp.sum(d_weight ** 2))\n",
    "    d_weight = from_dlpack(toDlpack(d_weight)).type(torch.FloatTensor)\n",
    "    \n",
    "    drop_weight_list = []\n",
    "    for name in model.state_dict():\n",
    "        if \"bias\" in name or \"norm\" in name or \"classifier\" in name:\n",
    "            continue\n",
    "        drop_weight = xp.random.normal(size=model.state_dict()[name].numel() + 1)\n",
    "        drop_weight /= (1e-12 + xp.max(xp.abs(drop_weight)))\n",
    "        drop_weight /= xp.sqrt(1e-6 + xp.sum(drop_weight ** 2))\n",
    "        drop_weight = from_dlpack(toDlpack(drop_weight)).type(torch.FloatTensor)\n",
    "        drop_weight_list += [drop_weight]\n",
    "    print(\"d weight shape:\",d_weight.shape)\n",
    "    print(\"d shape:\",d.shape)\n",
    "    for _ in range(num_simulations):\n",
    "        x_d = xi * d\n",
    "        w_d = xi * d_weight\n",
    "        w_d = torch.zeros(w_d.shape)\n",
    "        print(w_d.type())\n",
    "        x_d = Variable(x_d,requires_grad=True)\n",
    "        w_d = Variable(w_d,requires_grad=True)\n",
    "        w_d.retain_grad()\n",
    "        drop_d1_list = []\n",
    "        drop_d2_list = []\n",
    "        for ii in range(2):\n",
    "            drop_d1 = xi * (drop_weight_list[ii][:-1] + drop_weight_list[ii][-1])\n",
    "            drop_d2 = xi * drop_weight_list[ii][:-1]\n",
    "            drop_d1_list += [drop_d1]\n",
    "            drop_d2_list += [drop_d2] \n",
    "#         for name, module in model._modules.items():\n",
    "#             print(module.weight.data)\n",
    "#             module.weight.data = module.weight + w_d.reshape(module.weight.data.shape)\n",
    "#             print(module.weight.data)\n",
    "#             print(name)\n",
    "#             break\n",
    "        logit_d,d = delta_forward(model, logit, x, x_d, w_d, size_list)\n",
    "        logit_d,d = delta_forward_chainer(model, logit, x, x_d, w_d, size_list)\n",
    "        break\n",
    "        logit_drop1 = drop_forward(model, x, drop_d1_list)\n",
    "        logit_drop2 = drop_forward(model, x, drop_d2_list)       \n",
    "        #kl_loss = distance(logit.data, logit_d)\n",
    "        kl_loss = F.kl_div(F.softmax(logit),F.softmax(logit_d))\n",
    "#         kl_loss_drop1 = distance(logit.data, logit_drop1)\n",
    "#         kl_loss_drop2 = distance(logit.data, logit_drop2)\n",
    "        kl_loss_drop1 = F.kl_div(F.softmax(logit),F.softmax(logit_drop1))\n",
    "        kl_loss_drop2 = F.kl_div(F.softmax(logit),F.softmax(logit_drop2))\n",
    "        kl_loss.backward()\n",
    "        d,d_weight = x_d.grad,w_d.grad\n",
    "        model.zero_grad()\n",
    "        print(\"d:\",d)\n",
    "        print(\"d weight:\",d_weight)\n",
    "        break\n",
    "        d, d_weight = chainer.grad([kl_loss], [x_d, w_d], enable_double_backprop=False)\n",
    "        d = d / F.sqrt(F.sum(d ** 2, tuple(range(1, len(d.shape))), keepdims=True))\n",
    "        d_weight = d_weight / F.sqrt(F.sum(d_weight ** 2))\n",
    "        \n",
    "        layer1_drop1, layer4_drop1, layer7_drop1 = chainer.grad([kl_loss_drop1], drop_d1_list, enable_double_backprop=False)\n",
    "        layer1_drop2, layer4_drop2, layer7_drop2 = chainer.grad([kl_loss_drop2], drop_d2_list, enable_double_backprop=False)\n",
    "        \n",
    "        layer1_drop2 = F.reshape(F.sum(layer1_drop2), (1, 1))\n",
    "        layer4_drop2 = F.reshape(F.sum(layer4_drop2), (1, 1))\n",
    "        layer7_drop2 = F.reshape(F.sum(layer7_drop2), (1, 1))\n",
    "        drop_weight_list[0] = F.flatten(F.concat([F.reshape(layer1_drop1, (-1, 1)), layer1_drop2], axis=0))\n",
    "        drop_weight_list[1] = F.flatten(F.concat([F.reshape(layer4_drop1, (-1, 1)), layer4_drop2], axis=0))\n",
    "        drop_weight_list[2] = F.flatten(F.concat([F.reshape(layer7_drop1, (-1, 1)), layer7_drop2], axis=0))\n",
    "        drop_weight_list = [drop_weight_list[ii] / F.sqrt(F.sum(drop_weight_list[ii] ** 2)) for ii in range(3)]\n",
    "\n",
    "    return d,d_weight,drop_weight_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T07:54:33.520300Z",
     "start_time": "2020-07-06T07:54:33.183164Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import chainer\n",
    "import chainer.functions as F_chainer\n",
    "from chainer import Variable as V_chain\n",
    "import cupy as xp\n",
    "from cupy.core.dlpack import toDlpack\n",
    "from cupy.core.dlpack import fromDlpack\n",
    "from torch.utils.dlpack import to_dlpack\n",
    "from torch.utils.dlpack import from_dlpack\n",
    "from source.chainer_functions.loss import distance\n",
    "from networks import densenet\n",
    "from collections import OrderedDict\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "\n",
    "\n",
    "def delta_forward(model,logit, x, x_d, w_d, size_list):\n",
    "    #h = x + x_d\n",
    "    h = x\n",
    "    # densenet\n",
    "    k=1\n",
    "    h = F.conv2d(h,model.state_dict()['densenet121.features.conv0.weight'] + \n",
    "                 w_d[size_list[0]:size_list[k]].reshape(model.state_dict()['densenet121.features.conv0.weight'].shape),\n",
    "                 stride=2,padding=3)\n",
    "    h = F.max_pool2d(F.relu(F.batch_norm(h,model.state_dict()['densenet121.features.norm0.running_mean'],\n",
    "        model.state_dict()['densenet121.features.norm0.running_var'],\n",
    "        model.state_dict()['densenet121.features.norm0.weight'],\n",
    "        model.state_dict()['densenet121.features.norm0.bias'],training=True)),kernel_size=3, stride=2, padding=1)\n",
    "    #denseblock 1\n",
    "    denselayer_num = [6,12,24,16]\n",
    "    for i in range(1,5):\n",
    "        for j in range(1,denselayer_num[i-1]+1):\n",
    "        #denselayer 1\n",
    "            h_temp = h\n",
    "            dense_pre = 'densenet121.features.denseblock'+str(i)+'.denselayer'+str(j)\n",
    "            h = F.relu(F.batch_norm(h,model.state_dict()[dense_pre+'.norm1.running_mean'],\n",
    "                model.state_dict()[dense_pre+'.norm1.running_var'],\n",
    "                model.state_dict()[dense_pre+'.norm1.weight'],\n",
    "                model.state_dict()[dense_pre+'.norm1.bias'],training=True))\n",
    "            h = F.conv2d(h,model.state_dict()[dense_pre+'.conv1.weight'] + \n",
    "                w_d[size_list[k]:size_list[k+1]].reshape(model.state_dict()[dense_pre+'.conv1.weight'].shape))\n",
    "            k+=1\n",
    "            h = F.relu(F.batch_norm(h,model.state_dict()[dense_pre+'.norm2.running_mean'],\n",
    "                model.state_dict()[dense_pre+'.norm2.running_var'],\n",
    "                model.state_dict()[dense_pre+'.norm2.weight'],\n",
    "                model.state_dict()[dense_pre+'.norm2.bias'],training=True))\n",
    "            h = F.conv2d(h,model.state_dict()[dense_pre+'.conv2.weight'] + \n",
    "                w_d[size_list[k]:size_list[k+1]].reshape(model.state_dict()[dense_pre+'.conv2.weight'].shape),padding=1)\n",
    "            h = torch.cat([h_temp,h], 1)\n",
    "            k+=1\n",
    "        # transition1\n",
    "        if i < 4:\n",
    "            transi_pre = 'densenet121.features.transition' + str(i)\n",
    "            h = F.relu(F.batch_norm(h,model.state_dict()[transi_pre+'.norm.running_mean'],\n",
    "                model.state_dict()[transi_pre+'.norm.running_var'],\n",
    "                model.state_dict()[transi_pre+'.norm.weight'],\n",
    "                model.state_dict()[transi_pre+'.norm.bias'],training=True))\n",
    "            h = F.conv2d(h,model.state_dict()[transi_pre+'.conv.weight'] + \n",
    "                w_d[size_list[k]:size_list[k+1]].reshape(model.state_dict()[transi_pre+'.conv.weight'].shape))\n",
    "            k+=1\n",
    "            h = F.avg_pool2d(h,kernel_size=2, stride=2)\n",
    "    #norm 5\n",
    "    h = F.relu(F.batch_norm(h,model.state_dict()['densenet121.features.norm5.running_mean'],\n",
    "        model.state_dict()['densenet121.features.norm5.running_var'],\n",
    "        model.state_dict()['densenet121.features.norm5.weight'],\n",
    "        model.state_dict()['densenet121.features.norm5.bias'],training=True))\n",
    "    #h = F.avg_pool2d(h, kernel_size=7, stride=1).view(h.size(0), -1)\n",
    "    h = F.adaptive_avg_pool2d(h, (1, 1)).view(h.size(0), -1)\n",
    "    logit_perturb = F.linear(h,model.state_dict()['densenet121.classifier.0.weight'],model.state_dict()['densenet121.classifier.0.bias'])\n",
    "    \n",
    "    return logit_perturb\n",
    "\n",
    "def wcp_loss_torch(model, x, logit, epsilon=8., dr=0.5, num_simulations=1, xi=1e-6):\n",
    "    n_batch = x.shape[0]\n",
    "    size_list = [0]\n",
    "    size_sum = 0\n",
    "    for name in model.state_dict():\n",
    "        if \"bias\" in name or \"norm\" in name or \"classifier\" in name:\n",
    "            continue\n",
    "        size = model.state_dict()[name].numel()\n",
    "        size_sum += size \n",
    "        size_list += [size_sum]\n",
    "    d = xp.random.normal(size=x.shape)\n",
    "    d /= (1e-12 + xp.max(xp.abs(d),range(1, len(d.shape)), keepdims=True))\n",
    "    d /= xp.sqrt(1e-6 + xp.sum(d ** 2, range(1, len(d.shape)), keepdims=True))\n",
    "    d = from_dlpack(toDlpack(d)).type(torch.FloatTensor)\n",
    "    \n",
    "    d_weight = xp.random.normal(size=size_list[-1])\n",
    "    d_weight /= (1e-12 + xp.max(xp.abs(d_weight)))\n",
    "    d_weight /= xp.sqrt(1e-6 + xp.sum(d_weight ** 2))\n",
    "    d_weight = from_dlpack(toDlpack(d_weight)).type(torch.FloatTensor)\n",
    "    \n",
    "    drop_weight_list = []\n",
    "    for name in model.state_dict():\n",
    "        if \"bias\" in name or \"norm\" in name or \"classifier\" in name:\n",
    "            continue\n",
    "        drop_weight = xp.random.normal(size=model.state_dict()[name].numel() + 1)\n",
    "        drop_weight /= (1e-12 + xp.max(xp.abs(drop_weight)))\n",
    "        drop_weight /= xp.sqrt(1e-6 + xp.sum(drop_weight ** 2))\n",
    "        drop_weight = from_dlpack(toDlpack(drop_weight)).type(torch.FloatTensor)\n",
    "        drop_weight_list += [drop_weight]\n",
    "    print(\"d weight shape:\",d_weight.shape)\n",
    "    print(\"d shape:\",d.shape)\n",
    "    for _ in range(num_simulations):\n",
    "        x_d = xi * d\n",
    "        w_d = xi * d_weight\n",
    "        w_d = torch.zeros(w_d.shape)\n",
    "        print(w_d.type())\n",
    "        x_d = Variable(x_d,requires_grad=True)\n",
    "        w_d = Variable(w_d,requires_grad=True)\n",
    "        w_d.retain_grad()\n",
    "        drop_d1_list = []\n",
    "        drop_d2_list = []\n",
    "        for ii in range(2):\n",
    "            drop_d1 = xi * (drop_weight_list[ii][:-1] + drop_weight_list[ii][-1])\n",
    "            drop_d2 = xi * drop_weight_list[ii][:-1]\n",
    "            drop_d1_list += [drop_d1]\n",
    "            drop_d2_list += [drop_d2] \n",
    "#         for name, module in model._modules.items():\n",
    "#             print(module.weight.data)\n",
    "#             module.weight.data = module.weight + w_d.reshape(module.weight.data.shape)\n",
    "#             print(module.weight.data)\n",
    "#             print(name)\n",
    "#             break\n",
    "        logit_d = delta_forward(model, logit, x, x_d, w_d, size_list)\n",
    "        logp_hat = F.log_softmax(logit_d, dim=1)\n",
    "        kl_loss = F.kl_div(logp_hat,F.softmax(logit),reduction='batchmean')\n",
    "        kl_loss.backward()\n",
    "        d_weight = w_d.grad\n",
    "        model.zero_grad()\n",
    "        d_weight = d_weight / torch.sqrt(torch.sum(d_weight ** 2))\n",
    "        d_weight = epsilon * d_weight\n",
    "        logit_perturb = delta_forward(model, logit, x, x_d, d_weight, size_list)\n",
    "\n",
    "        #compute loss\n",
    "        logp_hat = F.log_softmax(logit_perturb, dim=1)\n",
    "        kl_loss = F.kl_div(logp_hat,F.softmax(logit),reduction='batchmean')\n",
    "    return kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T07:05:30.615980Z",
     "start_time": "2020-07-06T07:05:13.532809Z"
    }
   },
   "outputs": [],
   "source": [
    "model = densenet.densenet121(pretrained=True, drop_rate=0)\n",
    "image = torch.rand(3,3,224,224)\n",
    "out = model(image)\n",
    "_,_,output = wcp_loss_torch(model,image,out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# chest data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T07:54:21.026720Z",
     "start_time": "2020-07-16T07:54:21.021704Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "test_file = '../dataset/chest/test_list.xlsx'\n",
    "train_val_file = '../dataset/chest/train_val_list.xlsx'\n",
    "data_total = '../dataset/chest/Data_Entry_2017.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-12T07:52:01.498455Z",
     "start_time": "2020-07-12T07:52:00.642525Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T07:07:44.992842Z",
     "start_time": "2020-07-13T07:07:40.839144Z"
    }
   },
   "outputs": [],
   "source": [
    "test_list = pd.read_excel(test_file)\n",
    "train_list = pd.read_excel(train_val_file)\n",
    "df_test = pd.merge(test_list,df,on=['Image Index'])\n",
    "df_train_val = pd.merge(train_list,df,on=['Image Index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T02:48:53.097294Z",
     "start_time": "2020-07-13T02:48:53.089731Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25596\n",
      "86524\n"
     ]
    }
   ],
   "source": [
    "print(len(df_test))\n",
    "print(len(df_train_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T07:58:52.919043Z",
     "start_time": "2020-07-16T07:58:52.908856Z"
    }
   },
   "outputs": [],
   "source": [
    "#df_train_val = df_train_val.sample(frac=1.0)\n",
    "df_train = df_train_val[0:76524]\n",
    "df_val = df_train_val[76524:]\n",
    "df_train.to_csv(\"../dataset/chest/training_new.csv\",index=False)\n",
    "df_val.to_csv(\"../dataset/chest/validation_new.csv\",index=False)\n",
    "df_test.to_csv(\"../dataset/chest/testing_new.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T07:59:30.461597Z",
     "start_time": "2020-07-16T07:59:30.426348Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image Index</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>76524</th>\n",
       "      <td>00025513_002.png</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76525</th>\n",
       "      <td>00025513_003.png</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76526</th>\n",
       "      <td>00025513_004.png</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76527</th>\n",
       "      <td>00025513_005.png</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76528</th>\n",
       "      <td>00025513_006.png</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Image Index  1  2  3  4  5  6  7  8  9  10  11  12  13  14\n",
       "76524  00025513_002.png  0  0  1  1  1  0  0  0  0   0   0   0   0   0\n",
       "76525  00025513_003.png  0  0  0  1  0  0  0  0  0   0   0   0   0   0\n",
       "76526  00025513_004.png  0  0  1  1  0  0  0  0  0   0   0   0   0   0\n",
       "76527  00025513_005.png  0  0  1  1  0  0  0  0  0   1   0   0   0   0\n",
       "76528  00025513_006.png  0  0  0  0  0  0  0  0  0   0   0   0   0   0"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-13T07:07:48.218284Z",
     "start_time": "2020-07-13T07:07:47.553342Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train_val.to_csv(\"../dataset/chest/training_validation.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-12T07:55:04.459835Z",
     "start_time": "2020-07-12T07:55:04.234744Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-12T07:23:58.992963Z",
     "start_time": "2020-07-12T07:23:58.729772Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-12T07:24:02.339219Z",
     "start_time": "2020-07-12T07:24:02.302456Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image Index</th>\n",
       "      <th>Finding Labels</th>\n",
       "      <th>Follow-up #</th>\n",
       "      <th>Patient ID</th>\n",
       "      <th>Patient Age</th>\n",
       "      <th>Patient Gender</th>\n",
       "      <th>View Position</th>\n",
       "      <th>OriginalImage[Width</th>\n",
       "      <th>Height]</th>\n",
       "      <th>OriginalImagePixelSpacing[x</th>\n",
       "      <th>y]</th>\n",
       "      <th>Unnamed: 11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00000001_000.png</td>\n",
       "      <td>Cardiomegaly</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>58</td>\n",
       "      <td>M</td>\n",
       "      <td>PA</td>\n",
       "      <td>2682</td>\n",
       "      <td>2749</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.143</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00000001_001.png</td>\n",
       "      <td>Cardiomegaly|Emphysema</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>58</td>\n",
       "      <td>M</td>\n",
       "      <td>PA</td>\n",
       "      <td>2894</td>\n",
       "      <td>2729</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.143</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00000001_002.png</td>\n",
       "      <td>Cardiomegaly|Effusion</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>58</td>\n",
       "      <td>M</td>\n",
       "      <td>PA</td>\n",
       "      <td>2500</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.168</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00000002_000.png</td>\n",
       "      <td>No Finding</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>81</td>\n",
       "      <td>M</td>\n",
       "      <td>PA</td>\n",
       "      <td>2500</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.171</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00000003_000.png</td>\n",
       "      <td>Hernia</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>81</td>\n",
       "      <td>F</td>\n",
       "      <td>PA</td>\n",
       "      <td>2582</td>\n",
       "      <td>2991</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.143</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Image Index          Finding Labels  Follow-up #  Patient ID  \\\n",
       "0  00000001_000.png            Cardiomegaly            0           1   \n",
       "1  00000001_001.png  Cardiomegaly|Emphysema            1           1   \n",
       "2  00000001_002.png   Cardiomegaly|Effusion            2           1   \n",
       "3  00000002_000.png              No Finding            0           2   \n",
       "4  00000003_000.png                  Hernia            0           3   \n",
       "\n",
       "   Patient Age Patient Gender View Position  OriginalImage[Width  Height]  \\\n",
       "0           58              M            PA                 2682     2749   \n",
       "1           58              M            PA                 2894     2729   \n",
       "2           58              M            PA                 2500     2048   \n",
       "3           81              M            PA                 2500     2048   \n",
       "4           81              F            PA                 2582     2991   \n",
       "\n",
       "   OriginalImagePixelSpacing[x     y]  Unnamed: 11  \n",
       "0                        0.143  0.143          NaN  \n",
       "1                        0.143  0.143          NaN  \n",
       "2                        0.168  0.168          NaN  \n",
       "3                        0.171  0.171          NaN  \n",
       "4                        0.143  0.143          NaN  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-12T07:29:58.683343Z",
     "start_time": "2020-07-12T07:29:23.106649Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image Index</th>\n",
       "      <th>Finding Labels</th>\n",
       "      <th>Follow-up #</th>\n",
       "      <th>Patient ID</th>\n",
       "      <th>Patient Age</th>\n",
       "      <th>Patient Gender</th>\n",
       "      <th>View Position</th>\n",
       "      <th>OriginalImage[Width</th>\n",
       "      <th>Height]</th>\n",
       "      <th>OriginalImagePixelSpacing[x</th>\n",
       "      <th>y]</th>\n",
       "      <th>Unnamed: 11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>112119</th>\n",
       "      <td>00030805_000.png</td>\n",
       "      <td>No Finding</td>\n",
       "      <td>0</td>\n",
       "      <td>30805</td>\n",
       "      <td>27</td>\n",
       "      <td>M</td>\n",
       "      <td>PA</td>\n",
       "      <td>2048</td>\n",
       "      <td>2500</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.171</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Image Index Finding Labels  Follow-up #  Patient ID  Patient Age  \\\n",
       "112119  00030805_000.png     No Finding            0       30805           27   \n",
       "\n",
       "       Patient Gender View Position  OriginalImage[Width  Height]  \\\n",
       "112119              M            PA                 2048     2500   \n",
       "\n",
       "        OriginalImagePixelSpacing[x     y]  Unnamed: 11  \n",
       "112119                        0.171  0.171          NaN  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = df.apply(lambda row: row[u'Image Index'] in test_list, axis=1)\n",
    "df[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-12T07:30:30.709406Z",
     "start_time": "2020-07-12T07:30:30.690975Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         False\n",
       "1         False\n",
       "2         False\n",
       "3         False\n",
       "4         False\n",
       "5         False\n",
       "6         False\n",
       "7         False\n",
       "8         False\n",
       "9         False\n",
       "10        False\n",
       "11        False\n",
       "12        False\n",
       "13        False\n",
       "14        False\n",
       "15        False\n",
       "16        False\n",
       "17        False\n",
       "18        False\n",
       "19        False\n",
       "20        False\n",
       "21        False\n",
       "22        False\n",
       "23        False\n",
       "24        False\n",
       "25        False\n",
       "26        False\n",
       "27        False\n",
       "28        False\n",
       "29        False\n",
       "          ...  \n",
       "112090    False\n",
       "112091    False\n",
       "112092    False\n",
       "112093    False\n",
       "112094    False\n",
       "112095    False\n",
       "112096    False\n",
       "112097    False\n",
       "112098    False\n",
       "112099    False\n",
       "112100    False\n",
       "112101    False\n",
       "112102    False\n",
       "112103    False\n",
       "112104    False\n",
       "112105    False\n",
       "112106    False\n",
       "112107    False\n",
       "112108    False\n",
       "112109    False\n",
       "112110    False\n",
       "112111    False\n",
       "112112    False\n",
       "112113    False\n",
       "112114    False\n",
       "112115    False\n",
       "112116    False\n",
       "112117    False\n",
       "112118    False\n",
       "112119     True\n",
       "Length: 112120, dtype: bool"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-12T07:46:52.398890Z",
     "start_time": "2020-07-12T07:46:52.387942Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[u'Image Index'].str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-12T07:49:20.805429Z",
     "start_time": "2020-07-12T07:49:20.793648Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T13:55:38.397383Z",
     "start_time": "2020-07-15T13:55:38.391547Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T07:44:56.973197Z",
     "start_time": "2020-07-16T07:44:54.330354Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luckie/anaconda2/envs/py2/lib/python2.7/site-packages/ipykernel_launcher.py:1: FutureWarning: read_table is deprecated, use read_csv instead.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/home/luckie/anaconda2/envs/py2/lib/python2.7/site-packages/ipykernel_launcher.py:2: FutureWarning: read_table is deprecated, use read_csv instead.\n",
      "  \n",
      "/home/luckie/anaconda2/envs/py2/lib/python2.7/site-packages/ipykernel_launcher.py:3: FutureWarning: read_table is deprecated, use read_csv instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_table(\"../../paper_with_code/chexnet-master/dataset/train_1.txt\",sep=\" \",header=None)\n",
    "df_test = pd.read_table(\"../../paper_with_code/chexnet-master/dataset/test_1.txt\",sep=\" \",header=None)\n",
    "df_val = pd.read_table(\"../../paper_with_code/chexnet-master/dataset/val_1.txt\",sep=\" \",header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T07:45:48.634107Z",
     "start_time": "2020-07-16T07:45:48.612944Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.concat([df_train,df_test,df_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T14:01:24.791589Z",
     "start_time": "2020-07-15T14:01:23.939961Z"
    }
   },
   "outputs": [],
   "source": [
    "df.to_csv('test.txt', sep=' ', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T07:53:27.847030Z",
     "start_time": "2020-07-16T07:53:27.766934Z"
    }
   },
   "outputs": [],
   "source": [
    "df[0] = df[0].apply(lambda x: x.split(\"/\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T07:55:44.181050Z",
     "start_time": "2020-07-16T07:55:44.151596Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df.rename(columns={0:'Image Index'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T12:08:35.318832Z",
     "start_time": "2020-07-16T12:08:29.488453Z"
    }
   },
   "outputs": [],
   "source": [
    "test_list = pd.read_excel(test_file)\n",
    "train_list = pd.read_excel(train_val_file)\n",
    "df_test = pd.merge(test_list,df,on=['Image Index'])\n",
    "df_train_val = pd.merge(train_list,df,on=['Image Index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T08:01:47.806935Z",
     "start_time": "2020-07-16T08:01:47.800030Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T08:03:11.759206Z",
     "start_time": "2020-07-16T08:03:10.247941Z"
    }
   },
   "outputs": [],
   "source": [
    "df_test.to_csv('../../paper_with_code/chexnet-master/dataset/test_0.txt', sep=' ', index=False)\n",
    "df_val.to_csv('../../paper_with_code/chexnet-master/dataset/val_0.txt', sep=' ', index=False)\n",
    "df_train.to_csv('../../paper_with_code/chexnet-master/dataset/train_0.txt', sep=' ', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T08:53:53.394538Z",
     "start_time": "2020-07-16T08:53:52.339911Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train_shuffle = df_train.sample(frac=1.0)\n",
    "df_train_shuffle.to_csv('../../paper_with_code/chexnet-master/dataset/train_0_shuffle.txt', sep=' ', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T12:06:05.931731Z",
     "start_time": "2020-07-16T12:06:05.615405Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train_shuffle[:22424].to_csv('../../paper_with_code/chexnet-master/dataset/train_0_shuffle_frac02.txt', sep=' ', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T12:06:22.580574Z",
     "start_time": "2020-07-16T12:06:22.572854Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76524"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[:15305"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T12:08:14.993423Z",
     "start_time": "2020-07-16T12:08:14.691659Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train[:15305].to_csv('../../paper_with_code/chexnet-master/dataset/train_0_frac015.txt', sep=' ', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hip & skin data txt file  generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-24T12:20:37.424415Z",
     "start_time": "2020-07-24T12:20:36.918820Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-24T12:32:28.046790Z",
     "start_time": "2020-07-24T12:32:27.969758Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train_hip = pd.read_csv(\"../dataset/skin/training.csv\")\n",
    "df_train_hip[['image','0','1','2','3','4', '5', '6']].to_csv('../dataset/skin/training.txt',\n",
    "                                               sep=' ', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-24T12:27:32.683232Z",
     "start_time": "2020-07-24T12:27:32.581125Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "gist": {
   "data": {
    "description": "Untitled.ipynb",
    "public": false
   },
   "id": ""
  },
  "kernelspec": {
   "display_name": "Python [conda env:py2]",
   "language": "python",
   "name": "conda-env-py2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "272px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
