{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T01:37:19.384273Z",
     "start_time": "2020-06-16T01:37:15.169165Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "# from tensorboardX import SummaryWriter\n",
    "import shutil\n",
    "import argparse\n",
    "import logging\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import make_grid\n",
    "import pretrainedmodels\n",
    "\n",
    "from networks.models import DenseNet121,DenseNet161\n",
    "from utils import losses, ramps\n",
    "from utils.metrics import compute_AUCs\n",
    "from utils.metric_logger import MetricLogger\n",
    "from dataloaders import  dataset\n",
    "from dataloaders import chest_xray_14\n",
    "from dataloaders.dataset import TwoStreamBatchSampler\n",
    "from utils.util import get_timestamp\n",
    "from validation import epochVal, epochVal_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-10T08:38:59.689821Z",
     "start_time": "2020-06-10T08:38:59.625663Z"
    }
   },
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize([0.605, 0.605, 0.605],\n",
    "                                     [0.156, 0.156, 0.156])\n",
    "train_dataset = dataset.CheXpertDataset(root_dir='../dataset/hip_4cls/training_data/',\n",
    "                                        csv_file='../dataset/hip_4cls/training_sample_single_class.csv',\n",
    "                                        transform=dataset.TransformTwice(transforms.Compose([\n",
    "                                            transforms.Resize((224, 224)),\n",
    "                                            transforms.RandomAffine(degrees=10, translate=(0.02, 0.02)),\n",
    "                                            transforms.RandomHorizontalFlip(),\n",
    "                                            # transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),\n",
    "                                            # transforms.RandomRotation(10),\n",
    "                                            # transforms.RandomResizedCrop(224),\n",
    "                                            transforms.ToTensor(),\n",
    "                                            normalize,\n",
    "                                        ])))\n",
    "print(\"train_dataset len:\",len(train_dataset))\n",
    "train_dataset_num = len(train_dataset)\n",
    "labeled_num = int(train_dataset_num*0.2)\n",
    "print(\"labeled_num:\",labeled_num)\n",
    "labeled_idxs = list(range(labeled_num))unlabeled_idxs = list(range(labeled_num, train_dataset_num))\n",
    "batch_size = 16\n",
    "labeled_bs = 4\n",
    "batch_sampler = TwoStreamBatchSampler(labeled_idxs, unlabeled_idxs, batch_size, batch_size-labeled_bs)\n",
    "def worker_init_fn(worker_id):\n",
    "    random.seed(20000+worker_id)\n",
    "train_dataloader = DataLoader(dataset=train_dataset, batch_sampler=batch_sampler,\n",
    "                                  num_workers=8, pin_memory=True, worker_init_fn=worker_init_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-10T08:38:59.689821Z",
     "start_time": "2020-06-10T08:38:59.625663Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-10T08:39:58.708025Z",
     "start_time": "2020-06-10T08:39:54.907924Z"
    }
   },
   "outputs": [],
   "source": [
    "label_count = torch.LongTensor([-1])\n",
    "for i, (_,_, (image_batch, ema_image_batch), label_batch) in enumerate(train_dataloader):\n",
    "    print(image_batch[labeled_bs:].shape)\n",
    "    \n",
    "    time2 = time.time()\n",
    "    label_count = torch.cat((label_count, torch.argmax(label_batch[:labeled_bs], dim=1)), 0)\n",
    "    break\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-10T02:36:27.365213Z",
     "start_time": "2020-06-10T02:36:27.340280Z"
    }
   },
   "outputs": [],
   "source": [
    "total = [0,0,0,0]\n",
    "for i in range(1,len(label_count)):\n",
    "    label = label_count[i]\n",
    "    total[label]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-10T02:36:36.639352Z",
     "start_time": "2020-06-10T02:36:36.630716Z"
    }
   },
   "outputs": [],
   "source": [
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-10T02:36:27.494318Z",
     "start_time": "2020-06-10T02:36:27.489907Z"
    }
   },
   "outputs": [],
   "source": [
    "def _l2_normalize(d):\n",
    "    d_reshaped = d.view(d.shape[0], -1, *(1 for _ in range(d.dim() - 2)))\n",
    "    print(\"d_reshaped:\",d_reshaped.shape)\n",
    "    d /= torch.norm(d_reshaped, dim=1, keepdim=True) + 1e-8\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T09:14:03.362419Z",
     "start_time": "2020-06-09T09:14:03.349816Z"
    }
   },
   "outputs": [],
   "source": [
    "d = torch.rand([4,4,3,3]).sub(0.5)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T09:14:10.209953Z",
     "start_time": "2020-06-09T09:14:10.193177Z"
    }
   },
   "outputs": [],
   "source": [
    "_l2_normalize(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T12:27:33.609898Z",
     "start_time": "2020-05-29T12:27:33.589856Z"
    }
   },
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize([0.605, 0.605, 0.605],\n",
    "                                     [0.156, 0.156, 0.156])\n",
    "test_dataset = dataset.CheXpertDataset(root_dir='../dataset/hip/training_data/',\n",
    "                                      csv_file='../dataset/hip/testing_fold1.csv',\n",
    "                                      transform=transforms.Compose([\n",
    "                                          transforms.Resize((256, 256)),\n",
    "                                          transforms.ToTensor(),\n",
    "                                          normalize,\n",
    "                                      ]))\n",
    "test_dataloader = DataLoader(dataset=test_dataset, batch_size=1,\n",
    "                                shuffle=True, num_workers=8, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T17:17:32.732539Z",
     "start_time": "2020-05-29T17:17:32.700748Z"
    }
   },
   "outputs": [],
   "source": [
    "a = [1,2,10]\n",
    "test_dataset[200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T12:40:14.743461Z",
     "start_time": "2020-05-29T12:40:14.217113Z"
    }
   },
   "outputs": [],
   "source": [
    "a = torch.load(\"../model/0528_hip_test_label_rate09/checkpoint/epoch_12.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T12:55:32.798518Z",
     "start_time": "2020-05-29T12:55:32.792555Z"
    }
   },
   "outputs": [],
   "source": [
    "a.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T12:58:24.560103Z",
     "start_time": "2020-05-29T12:58:23.758560Z"
    }
   },
   "outputs": [],
   "source": [
    "net = DenseNet121(out_size=3, mode='U-Ones', drop_rate=0.2)\n",
    "net = torch.nn.DataParallel(net)\n",
    "net = net.cuda()\n",
    "checkpoint = torch.load(\"../model/0528_hip_test_label_rate09/checkpoint/epoch_12.pth\")\n",
    "net.load_state_dict(checkpoint['state_dict'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T13:23:10.901221Z",
     "start_time": "2020-05-29T13:23:01.367348Z"
    }
   },
   "outputs": [],
   "source": [
    "net.eval()\n",
    "count = 0\n",
    "total = 0\n",
    "for i, (study, _, image, label) in enumerate(test_dataloader):\n",
    "    #print(\"image:\",study)\n",
    "    image,label = image.cuda(),label.cuda()\n",
    "    gt_label = torch.max(label, 1)[1]\n",
    "    if gt_label == 1:\n",
    "        total +=1\n",
    "        print(\"gt label:\",gt_label)\n",
    "        _,output = net(image)\n",
    "        pred_label = torch.max(output, 1)[1]\n",
    "        if pred_label == gt_label:\n",
    "            count+=1\n",
    "        print(\"pred_label:\",pred_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T13:21:13.962734Z",
     "start_time": "2020-05-29T13:21:13.954344Z"
    }
   },
   "outputs": [],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T13:17:04.241069Z",
     "start_time": "2020-05-29T13:17:04.232671Z"
    }
   },
   "outputs": [],
   "source": [
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T13:21:26.269959Z",
     "start_time": "2020-05-29T13:21:26.264360Z"
    }
   },
   "outputs": [],
   "source": [
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T05:05:13.592348Z",
     "start_time": "2020-05-31T05:05:13.541297Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "df = pd.read_csv(\"../dataset/hip_4cls/training.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T05:03:50.615647Z",
     "start_time": "2020-05-31T05:03:50.604461Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df.sample(frac = 0.025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T05:03:54.775963Z",
     "start_time": "2020-05-31T05:03:54.768960Z"
    }
   },
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T05:04:09.072557Z",
     "start_time": "2020-05-31T05:04:09.057197Z"
    }
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"../dataset/hip_4cls/training_frac002.csv\",index=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get  traing data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-30T05:22:55.010559Z",
     "start_time": "2020-05-30T05:22:55.005900Z"
    }
   },
   "outputs": [],
   "source": [
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T02:33:58.197584Z",
     "start_time": "2020-05-31T02:33:57.276866Z"
    }
   },
   "outputs": [],
   "source": [
    "data_list = glob(\"../../hipX_largedata/data/512_2-1_train_3cls_new/*/*/*png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T02:34:11.629166Z",
     "start_time": "2020-05-31T02:34:11.622515Z"
    }
   },
   "outputs": [],
   "source": [
    "len(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T02:35:41.737135Z",
     "start_time": "2020-05-31T02:35:18.657998Z"
    }
   },
   "outputs": [],
   "source": [
    "# for semi-supervised\n",
    "df =pd.DataFrame(columns=('image','disease'))\n",
    "for img in data_list:\n",
    "    img_name = img.split(\"/\")[-1]\n",
    "    label = img.split(\"/\")[-2][0]\n",
    "    df=df.append(pd.DataFrame({'image':[img_name],'disease':[label]}),ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T02:36:45.625264Z",
     "start_time": "2020-05-31T02:36:45.551908Z"
    }
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"../../semi_supervised_cls/dataset/hip_4cls/training2.csv\",index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T02:40:29.851680Z",
     "start_time": "2020-05-31T02:40:29.801900Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../../semi_supervised_cls/dataset/hip_4cls/training2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T02:40:41.547137Z",
     "start_time": "2020-05-31T02:40:41.529568Z"
    }
   },
   "outputs": [],
   "source": [
    "col = 'disease'\n",
    "data = df\n",
    "data[col] = data[col].astype('category')#转换成数据类别类型，pandas用法\n",
    "dummy = pd.get_dummies(data[col])  #get_dummies为pandas里面求哑变量的包\n",
    "#dummy = dummy.add_prefix('{}#'.format(col)) #add_prefix为加上前缀\n",
    "#data.drop(col,axis = 1,inplace = True)\n",
    "data = data.join(dummy) #index即为userid，所以可以用join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T02:42:18.945256Z",
     "start_time": "2020-05-31T02:42:18.938157Z"
    }
   },
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T02:41:50.311795Z",
     "start_time": "2020-05-31T02:41:50.297741Z"
    }
   },
   "outputs": [],
   "source": [
    "order = ['image',0,1,2,3,'disease']\n",
    "data = data[order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T02:44:29.610708Z",
     "start_time": "2020-05-31T02:44:29.342159Z"
    }
   },
   "outputs": [],
   "source": [
    "df = data.sample(frac=1.0).reset_index(drop=True)\n",
    "df[0:19032].to_csv(\"../dataset/hip_4cls/training.csv\",index=0)\n",
    "df[19032:21750].to_csv(\"../dataset/hip_4cls/validation.csv\",index=0)\n",
    "df[21750:].to_csv(\"../dataset/hip_4cls/testing.csv\",index=0)\n",
    "#df_train = data.sample(frac=0.7).reset_index(drop=True)\n",
    "#df_train = data.sample(frac=0.7).reset_index(drop=True)\n",
    "#df_train = data.sample(frac=0.7).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"../../semi_supervised_cls/dataset/hip/testing_fold2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get diabetics data training file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T14:15:26.973918Z",
     "start_time": "2020-06-01T14:15:26.933844Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../dataset/diabetics/trainLabels_cropped.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T14:15:34.349212Z",
     "start_time": "2020-06-01T14:15:34.333577Z"
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T14:16:01.518818Z",
     "start_time": "2020-06-01T14:16:01.498911Z"
    }
   },
   "outputs": [],
   "source": [
    "col = 'level'\n",
    "data = df\n",
    "data[col] = data[col].astype('category')#转换成数据类别类型，pandas用法\n",
    "dummy = pd.get_dummies(data[col])  #get_dummies为pandas里面求哑变量的包\n",
    "#dummy = dummy.add_prefix('{}#'.format(col)) #add_prefix为加上前缀\n",
    "#data.drop(col,axis = 1,inplace = True)\n",
    "data = data.join(dummy) #index即为userid，所以可以用join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T14:16:09.836706Z",
     "start_time": "2020-06-01T14:16:09.817465Z"
    }
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T14:17:26.771952Z",
     "start_time": "2020-06-01T14:17:26.746184Z"
    }
   },
   "outputs": [],
   "source": [
    "data['image'] = data['image'].apply(lambda x: x+\".jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T15:43:06.401971Z",
     "start_time": "2020-06-01T15:43:06.394495Z"
    }
   },
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T14:18:41.986396Z",
     "start_time": "2020-06-01T14:18:41.974336Z"
    }
   },
   "outputs": [],
   "source": [
    "order = ['image',0,1,2,3,4,'level']\n",
    "data = data[order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T15:44:14.968896Z",
     "start_time": "2020-06-01T15:44:14.699072Z"
    }
   },
   "outputs": [],
   "source": [
    "df = data.sample(frac=1.0).reset_index(drop=True)\n",
    "df[0:24576].to_csv(\"../dataset/diabetics/training.csv\",index=0)\n",
    "df[24576:28086].to_csv(\"../dataset/diabetics/validation.csv\",index=0)\n",
    "df[28086:].to_csv(\"../dataset/diabetics/testing.csv\",index=0)\n",
    "#df_train = data.sample(frac=0.7).reset_index(drop=True)\n",
    "#df_train = data.sample(frac=0.7).reset_index(drop=True)\n",
    "#df_train = data.sample(frac=0.7).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test bnm loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T09:16:34.131571Z",
     "start_time": "2020-06-02T09:16:34.123638Z"
    }
   },
   "outputs": [],
   "source": [
    "A = torch.FloatTensor([[0.6,0.2,0.2],[0.3,0.3,0.4],[1,0,0],[0.4,0.4,0.2]]) +0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T09:16:36.838305Z",
     "start_time": "2020-06-02T09:16:36.833826Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "B = -1.0 * A*torch.log(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T09:16:39.124415Z",
     "start_time": "2020-06-02T09:16:39.117027Z"
    }
   },
   "outputs": [],
   "source": [
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T09:16:42.177103Z",
     "start_time": "2020-06-02T09:16:42.171615Z"
    }
   },
   "outputs": [],
   "source": [
    "C = B.sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T09:16:45.185982Z",
     "start_time": "2020-06-02T09:16:45.178476Z"
    }
   },
   "outputs": [],
   "source": [
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T09:16:49.615778Z",
     "start_time": "2020-06-02T09:16:49.611305Z"
    }
   },
   "outputs": [],
   "source": [
    "index = C.argsort(descending=True)[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T09:16:52.652484Z",
     "start_time": "2020-06-02T09:16:52.646651Z"
    }
   },
   "outputs": [],
   "source": [
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T09:17:15.704759Z",
     "start_time": "2020-06-02T09:17:15.694709Z"
    }
   },
   "outputs": [],
   "source": [
    "A[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sample from every class independent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-10T01:20:54.390886Z",
     "start_time": "2020-06-10T01:20:54.342334Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "df = pd.read_csv('../dataset/hip_4cls/training_sample_single_class.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-10T01:26:56.952497Z",
     "start_time": "2020-06-10T01:26:56.939167Z"
    }
   },
   "outputs": [],
   "source": [
    "df_0 = df[0:4218]\n",
    "df_1 = df[4218:8266]\n",
    "df_2 = df[8266:15149]\n",
    "df_3 = df[15149:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-10T01:32:36.734192Z",
     "start_time": "2020-06-10T01:32:36.714379Z"
    }
   },
   "outputs": [],
   "source": [
    "df_0 = df_0.sample(frac=1.0).reset_index(drop=True)\n",
    "df_1 = df_1.sample(frac=1.0).reset_index(drop=True)\n",
    "df_2 = df_2.sample(frac=1.0).reset_index(drop=True)\n",
    "df_3 = df_3.sample(frac=1.0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-10T01:32:41.448714Z",
     "start_time": "2020-06-10T01:32:41.439958Z"
    }
   },
   "outputs": [],
   "source": [
    "df_new = pd.concat([df_0[0:len(df_0)*0.1*x],df_1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-10T01:32:42.014337Z",
     "start_time": "2020-06-10T01:32:42.001121Z"
    }
   },
   "outputs": [],
   "source": [
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-10T02:11:44.053961Z",
     "start_time": "2020-06-10T02:11:43.815414Z"
    }
   },
   "outputs": [],
   "source": [
    "df_new = pd.DataFrame(columns=['image','0','1','2','3','disease'])\n",
    "for i in range(0,50):\n",
    "    left = int()\n",
    "    r = 0.02\n",
    "    df_cat = pd.concat([df_0[int(len(df_0)*r*i):int(len(df_0)*r*(i+1))],\n",
    "                    df_1[int(len(df_1)*r*i):int(len(df_1)*r*(i+1))],\n",
    "                   df_2[int(len(df_2)*r*i):int(len(df_2)*r*(i+1))],\n",
    "                   df_3[int(len(df_3)*r*i):int(len(df_3)*r*(i+1))]])\n",
    "    df_cat = df_cat.sample(frac=1.0).reset_index(drop=True)\n",
    "    df_new = pd.concat([df_new,df_cat])\n",
    "    #df_new = df_new.sample(frac=1.0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-10T02:13:14.946854Z",
     "start_time": "2020-06-10T02:13:14.925108Z"
    }
   },
   "outputs": [],
   "source": [
    "df_new.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-10T02:16:34.370903Z",
     "start_time": "2020-06-10T02:16:34.276955Z"
    }
   },
   "outputs": [],
   "source": [
    "df_new.to_csv(\"../dataset/hip_4cls/training_sample_single_class.csv\",index=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-scale densenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-11T13:17:07.578346Z",
     "start_time": "2020-06-11T13:17:07.554199Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from networks import densenet\n",
    "from collections import OrderedDict\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "\n",
    "\n",
    "class _DenseLayer(nn.Sequential):\n",
    "    def __init__(self, num_input_features, growth_rate, bn_size, drop_rate):\n",
    "        super(_DenseLayer, self).__init__()\n",
    "        self.add_module('norm1', nn.BatchNorm2d(num_input_features)),\n",
    "        self.add_module('relu1', nn.ReLU(inplace=True)),\n",
    "        self.add_module('conv1', nn.Conv2d(num_input_features, bn_size *\n",
    "                        growth_rate, kernel_size=1, stride=1, bias=False)),\n",
    "        self.add_module('norm2', nn.BatchNorm2d(bn_size * growth_rate)),\n",
    "        self.add_module('relu2', nn.ReLU(inplace=True)),\n",
    "        self.add_module('conv2', nn.Conv2d(bn_size * growth_rate, growth_rate,\n",
    "                        kernel_size=3, stride=1, padding=1, bias=False)),\n",
    "        self.drop_rate = drop_rate\n",
    "        self.drop_layer = nn.Dropout(p=drop_rate)\n",
    "    def forward(self, x):\n",
    "        new_features = super(_DenseLayer, self).forward(x)\n",
    "        # if self.drop_rate > 0:\n",
    "        #     print (self.drop_rate)\n",
    "        #     new_features = self.drop_layer(new_features)\n",
    "        return torch.cat([x, new_features], 1)\n",
    "\n",
    "\n",
    "class _DenseBlock(nn.Sequential):\n",
    "    def __init__(self, num_layers, num_input_features, bn_size, growth_rate, drop_rate):\n",
    "        super(_DenseBlock, self).__init__()\n",
    "        for i in range(num_layers):\n",
    "            layer = _DenseLayer(num_input_features + i * growth_rate, growth_rate, bn_size, drop_rate)\n",
    "            self.add_module('denselayer%d' % (i + 1), layer)\n",
    "\n",
    "\n",
    "class _Transition(nn.Sequential):\n",
    "    def __init__(self, num_input_features, num_output_features):\n",
    "        super(_Transition, self).__init__()\n",
    "        self.add_module('norm', nn.BatchNorm2d(num_input_features))\n",
    "        self.add_module('relu', nn.ReLU(inplace=True))\n",
    "        self.add_module('conv', nn.Conv2d(num_input_features, num_output_features,\n",
    "                                          kernel_size=1, stride=1, bias=False))\n",
    "        self.add_module('pool', nn.AvgPool2d(kernel_size=2, stride=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T01:26:05.214653Z",
     "start_time": "2020-06-12T01:26:05.180219Z"
    }
   },
   "outputs": [],
   "source": [
    "class DenseNetMultiScale(nn.Module):\n",
    "    r\"\"\"Densenet-BC model class, based on\n",
    "    `\"Densely Connected Convolutional Networks\" <https://arxiv.org/pdf/1608.06993.pdf>`_\n",
    "    Args:\n",
    "        growth_rate (int) - how many filters to add each layer (`k` in paper)\n",
    "        block_config (list of 4 ints) - how many layers in each pooling block\n",
    "        num_init_features (int) - the number of filters to learn in the first convolution layer\n",
    "        bn_size (int) - multiplicative factor for number of bottle neck layers\n",
    "          (i.e. bn_size * k features in the bottleneck layer)\n",
    "        drop_rate (float) - dropout rate after each dense layer\n",
    "        num_classes (int) - number of classification classes\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, growth_rate=32, block_config=(6, 12, 24, 16),\n",
    "                 num_init_features=64, bn_size=4, drop_rate=0, num_classes=1000):\n",
    "\n",
    "        super(DenseNetMultiScale, self).__init__()\n",
    "\n",
    "        # First convolution\n",
    "        self.features = nn.Sequential(OrderedDict([\n",
    "            ('conv0', nn.Conv2d(3, num_init_features, kernel_size=7, stride=2, padding=3, bias=False)),\n",
    "            ('norm0', nn.BatchNorm2d(num_init_features)),\n",
    "            ('relu0', nn.ReLU(inplace=True)),\n",
    "            ('pool0', nn.MaxPool2d(kernel_size=3, stride=2, padding=1)),\n",
    "        ]))\n",
    "\n",
    "        # Each denseblock\n",
    "        num_features = num_init_features\n",
    "        self.denseblock_list = []\n",
    "        for i, num_layers in enumerate(block_config):\n",
    "            block = _DenseBlock(num_layers=num_layers, num_input_features=num_features,\n",
    "                                bn_size=bn_size, growth_rate=growth_rate, drop_rate=drop_rate)\n",
    "            self.features.add_module('denseblock%d' % (i + 1), block)\n",
    "            num_features = num_features + num_layers * growth_rate\n",
    "            if i != len(block_config) - 1:\n",
    "                trans = _Transition(num_input_features=num_features, num_output_features=num_features // 2)\n",
    "                self.features.add_module('transition%d' % (i + 1), trans)\n",
    "                num_features = num_features // 2\n",
    "            if i != len(block_config) - 1:\n",
    "                self.denseblock_list.append(nn.Sequential(block,trans))\n",
    "            else:\n",
    "                self.denseblock_list.append(nn.Sequential(block))\n",
    "#         self.denseblock1 = self.denseblock_list[0]\n",
    "#         self.denseblock2 = self.denseblock_list[1]\n",
    "#         self.denseblock3 = self.denseblock_list[2]\n",
    "#         self.denseblock4 = self.denseblock_list[3]\n",
    "        # Final batch norm\n",
    "        self.features.add_module('norm5', nn.BatchNorm2d(num_features))\n",
    "        # Linear layer\n",
    "        self.classifier = nn.Linear(num_features, num_classes)\n",
    "\n",
    "        # Official init from torch repo.\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features.conv0(x)\n",
    "        x = self.features.norm0(x)\n",
    "        x = self.features.relu0(x)\n",
    "        x = self.features.pool0(x)\n",
    "        x = self.features.denseblock1(x)\n",
    "        fea1 = self.features.transition1(x)\n",
    "        fea2 = self.features.denseblock2(fea1)\n",
    "        fea2 = self.features.transition2(fea2)\n",
    "        fea3 = self.features.denseblock3(fea2)\n",
    "        fea3 = self.features.transition3(fea3)\n",
    "        features = self.features.denseblock4(fea3)\n",
    "        print(\"fea1 shape:\",fea1.shape)\n",
    "        print(\"fea2 shape:\",fea2.shape)\n",
    "        print(\"fea3 shape:\",fea3.shape)\n",
    "        out = F.relu(features, inplace=True)\n",
    "        fea_out2 = F.adaptive_avg_pool2d(fea2, (1, 1)).view(fea2.size(0), -1)\n",
    "        fea_out3 = F.adaptive_avg_pool2d(fea3, (1, 1)).view(fea3.size(0), -1)\n",
    "        fea_out = F.adaptive_avg_pool2d(out, (1, 1)).view(features.size(0), -1)\n",
    "        print(fea_out.size())\n",
    "        out = self.classifier(fea_out)\n",
    "        return [fea_out2,fea_out3,fea_out],out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-13T07:25:46.133291Z",
     "start_time": "2020-06-13T07:25:46.102214Z"
    }
   },
   "outputs": [],
   "source": [
    "class DenseNet121MultiScale(nn.Module):\n",
    "    \"\"\"Model modified.\n",
    "    The architecture of our model is the same as standard DenseNet121\n",
    "    except the classifier layer which has an additional sigmoid function.\n",
    "    \"\"\"\n",
    "    def __init__(self, out_size,drop_rate=0):\n",
    "        super(DenseNet121MultiScale, self).__init__()\n",
    "        self.densenet121 = densenet.densenet121(pretrained=True, drop_rate=drop_rate)\n",
    "        num_ftrs = self.densenet121.classifier.in_features\n",
    "        self.densenet121.classifier = nn.Sequential(\n",
    "            nn.Linear(num_ftrs, out_size),\n",
    "            #nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        # Official init from torch repo.\n",
    "        for m in self.densenet121.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        self.drop_rate = drop_rate\n",
    "        self.drop_layer = nn.Dropout(p=drop_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.densenet121.features.conv0(x)\n",
    "        x = self.densenet121.features.norm0(x)\n",
    "        x = self.densenet121.features.relu0(x)\n",
    "        x = self.densenet121.features.pool0(x)\n",
    "        x = self.densenet121.features.denseblock1(x)\n",
    "        fea1 = self.densenet121.features.transition1(x)\n",
    "        fea2 = self.densenet121.features.denseblock2(fea1)\n",
    "        fea2 = self.densenet121.features.transition2(fea2)\n",
    "        fea3 = self.densenet121.features.denseblock3(fea2)\n",
    "        fea3 = self.densenet121.features.transition3(fea3)\n",
    "        print(\"fea1 shape:\",fea1.shape)\n",
    "        print(\"fea2 shape:\",fea2.shape)\n",
    "        print(\"fea3 shape:\",fea3.shape)\n",
    "        features = self.densenet121.features.denseblock4(fea3)\n",
    "        features = self.densenet121.features.norm5(features)\n",
    "        out = F.relu(features, inplace=True) \n",
    "        fea_out2 = F.adaptive_avg_pool2d(fea2, (1, 1)).view(fea2.size(0), -1)\n",
    "        fea_out3 = F.adaptive_avg_pool2d(fea3, (1, 1)).view(fea3.size(0), -1)\n",
    "        out = F.adaptive_avg_pool2d(out, (1, 1)).view(features.size(0), -1)\n",
    "        if self.drop_rate > 0:\n",
    "            out = self.drop_layer(out)\n",
    "        self.activations = out\n",
    "        out = self.densenet121.classifier(out)\n",
    "            \n",
    "        return features,[fea_out2,fea_out3,self.activations], out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-13T07:25:49.797975Z",
     "start_time": "2020-06-13T07:25:49.783782Z"
    }
   },
   "outputs": [],
   "source": [
    "class DenseNet121(nn.Module):\n",
    "    \"\"\"Model modified.\n",
    "    The architecture of our model is the same as standard DenseNet121\n",
    "    except the classifier layer which has an additional sigmoid function.\n",
    "    \"\"\"\n",
    "    def __init__(self, out_size, mode, drop_rate=0):\n",
    "        super(DenseNet121, self).__init__()\n",
    "        assert mode in ('U-Ones', 'U-Zeros', 'U-MultiClass')\n",
    "        self.densenet121 = densenet.densenet121(pretrained=True, drop_rate=drop_rate)\n",
    "        num_ftrs = self.densenet121.classifier.in_features\n",
    "        if mode in ('U-Ones', 'U-Zeros'):\n",
    "            self.densenet121.classifier = nn.Sequential(\n",
    "                nn.Linear(num_ftrs, out_size),\n",
    "                #nn.Sigmoid()\n",
    "            )\n",
    "        elif mode in ('U-MultiClass', ):\n",
    "            self.densenet121.classifier = None\n",
    "            self.densenet121.Linear_0 = nn.Linear(num_ftrs, out_size)\n",
    "            self.densenet121.Linear_1 = nn.Linear(num_ftrs, out_size)\n",
    "            self.densenet121.Linear_u = nn.Linear(num_ftrs, out_size)\n",
    "            \n",
    "        self.mode = mode\n",
    "        \n",
    "        # Official init from torch repo.\n",
    "        for m in self.densenet121.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        self.drop_rate = drop_rate\n",
    "        self.drop_layer = nn.Dropout(p=drop_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        ori = x\n",
    "        features = self.densenet121.features(x)\n",
    "        out = F.relu(features, inplace=True)\n",
    "        \n",
    "        \n",
    "        out = F.adaptive_avg_pool2d(out, (1, 1)).view(features.size(0), -1)\n",
    "\n",
    "        if self.drop_rate > 0:\n",
    "            out = self.drop_layer(out)\n",
    "        self.activations = out\n",
    "        out = self.densenet121.classifier(out)\n",
    "            \n",
    "        return features,self.activations, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-13T07:25:53.833231Z",
     "start_time": "2020-06-13T07:25:53.104586Z"
    }
   },
   "outputs": [],
   "source": [
    "#model = DenseNetMultiScale(num_init_features=64, growth_rate=32, block_config=(6, 12, 24, 16))\n",
    "model1 = DenseNet121MultiScale(out_size=4, drop_rate=0)\n",
    "model2 = DenseNet121(out_size=4, mode='U-Ones', drop_rate=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-13T07:26:10.530904Z",
     "start_time": "2020-06-13T07:26:09.407001Z"
    }
   },
   "outputs": [],
   "source": [
    "image = torch.rand(3,3,256,256)\n",
    "ori1,fea1,out1 = model1(image)\n",
    "ori2,fea2,out2 = model2(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-14T08:11:49.869620Z",
     "start_time": "2020-06-14T08:11:49.856437Z"
    }
   },
   "outputs": [],
   "source": [
    "fea1[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T06:15:03.508866Z",
     "start_time": "2020-06-12T06:15:03.482002Z"
    }
   },
   "outputs": [],
   "source": [
    "ori2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-11T14:18:30.276543Z",
     "start_time": "2020-06-11T14:18:30.106922Z"
    }
   },
   "outputs": [],
   "source": [
    "model_urls = {\n",
    "    'densenet121': 'https://download.pytorch.org/models/densenet121-a639ec97.pth',\n",
    "    'densenet169': 'https://download.pytorch.org/models/densenet169-b2777c0a.pth',\n",
    "    'densenet201': 'https://download.pytorch.org/models/densenet201-c1103571.pth',\n",
    "    'densenet161': 'https://download.pytorch.org/models/densenet161-8d451a50.pth',\n",
    "}\n",
    "pattern = re.compile(\n",
    "    r'^(.*denselayer\\d+\\.(?:norm|relu|conv))\\.((?:[12])\\.(?:weight|bias|running_mean|running_var))$')\n",
    "state_dict = model_zoo.load_url(model_urls['densenet121'])\n",
    "for key in list(state_dict.keys()):\n",
    "    res = pattern.match(key)\n",
    "    if res:\n",
    "        new_key = res.group(1) + res.group(2)\n",
    "        state_dict[new_key] = state_dict[key]\n",
    "        del state_dict[key]\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-11T12:16:15.588873Z",
     "start_time": "2020-06-11T12:16:14.826419Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-11T12:16:29.804317Z",
     "start_time": "2020-06-11T12:16:29.795135Z"
    }
   },
   "outputs": [],
   "source": [
    "fea[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test BNM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T08:14:36.032835Z",
     "start_time": "2020-06-16T08:14:36.014145Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "# from tensorboardX import SummaryWriter\n",
    "import shutil\n",
    "import argparse\n",
    "import logging\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import make_grid\n",
    "import pretrainedmodels\n",
    "\n",
    "from networks.models import DenseNet121,DenseNet161\n",
    "from utils import losses, ramps\n",
    "from utils.metrics import compute_AUCs\n",
    "from utils.metric_logger import MetricLogger\n",
    "from dataloaders import  dataset\n",
    "from dataloaders import chest_xray_14\n",
    "from dataloaders.dataset import TwoStreamBatchSampler\n",
    "from utils.util import get_timestamp\n",
    "from validation import epochVal, epochVal_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T08:15:08.628890Z",
     "start_time": "2020-06-16T08:15:08.609069Z"
    }
   },
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                     [0.229, 0.224, 0.225])\n",
    "resize = 224\n",
    "test_dataset = dataset.CheXpertDataset(root_dir='../dataset/skin/training_data/',\n",
    "                                        csv_file='../dataset/skin/testing.csv',\n",
    "                                        transform=transforms.Compose([\n",
    "                                              transforms.Resize((resize, resize)),\n",
    "                                              transforms.ToTensor(),\n",
    "                                              normalize,\n",
    "                                        ]))\n",
    "print(\"train_dataset len:\",len(test_dataset))\n",
    "test_dataloader = DataLoader(dataset=test_dataset, batch_size=1,\n",
    "                                shuffle=False, num_workers=8, pin_memory=True)#, worker_init_fn=worker_init_fn)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-18T04:24:24.016716Z",
     "start_time": "2020-06-18T04:24:22.451456Z"
    }
   },
   "outputs": [],
   "source": [
    "model_baseline = DenseNet121(out_size=7, mode='U-Ones', drop_rate=0.2)\n",
    "model_bnm = DenseNet121(out_size=7, mode='U-Ones', drop_rate=0.2)\n",
    "model_baseline = torch.nn.DataParallel(model_baseline).cuda()\n",
    "model_bnm = torch.nn.DataParallel(model_bnm).cuda()\n",
    "#checkpoint_baseline = torch.load(\"../model/skin_frac01_baseline/epoch_92.pth\")\n",
    "checkpoint_baseline = torch.load(\"../model/skin_frac01_mt/epoch_31.pth\")\n",
    "checkpoint_bnm = torch.load(\"../model/skin_frac01_mt_bnm/epoch_68.pth\")\n",
    "model_baseline.load_state_dict(checkpoint_baseline['state_dict'])\n",
    "model_bnm.load_state_dict(checkpoint_bnm['state_dict'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-18T04:29:43.629224Z",
     "start_time": "2020-06-18T04:24:35.626851Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_baseline.eval()\n",
    "model_bnm.eval()\n",
    "cnt_total = 0\n",
    "cnt_06 = 0\n",
    "cnt_07 = 0\n",
    "total = [0,0,0,0,0,0,0]\n",
    "correct_baseline = [0,0,0,0,0,0,0]\n",
    "correct_bnm = [0,0,0,0,0,0,0]\n",
    "pred_baseline = [0,0,0,0,0,0,0]\n",
    "pre_bnm = [0 for i in range(7)]\n",
    "gt_study   = {}\n",
    "pred_study = {}\n",
    "studies    = []\n",
    "prob_list = [0.5,0.6,0.7,0.8,0.9,0.95,0.97,0.98,0.99]\n",
    "cnt_prob_total = [0 for i in range(9)]\n",
    "cnt_prob_correct = [0 for i in range(9)]\n",
    "print(\"eval\")\n",
    "with torch.no_grad():\n",
    "    for i,(study, _, image, label) in enumerate(test_dataloader):\n",
    "        image, label = image.cuda(), label.cuda()\n",
    "        _,output_baseline = model_baseline(image)\n",
    "        _,output_bnm = model_bnm(image)\n",
    "        output_baseline = F.softmax(output_baseline,dim=1)\n",
    "        output_bnm = F.softmax(output_bnm,dim=1)\n",
    "        label = torch.max(label, 1)[1]\n",
    "        label_baseline = torch.max(output_baseline, 1)[1]\n",
    "        label_bnm = torch.max(output_bnm, 1)[1]\n",
    "        pred_baseline[label_baseline]+=1\n",
    "        pre_bnm[label_bnm]+=1\n",
    "        total[label]+=1\n",
    "        if label == label_bnm:\n",
    "            correct_bnm[label]+= 1\n",
    "        if label == label_baseline:\n",
    "            correct_baseline[label] += 1\n",
    "\n",
    "        output_baseline_np = output_baseline.cpu().detach().numpy()\n",
    "        output_bnm_np = output_bnm.cpu().detach().numpy()\n",
    "        if label_baseline == 1:\n",
    "            cnt_total += 1\n",
    "            for j,prob in enumerate(prob_list):\n",
    "                if np.max(output_baseline_np)<=prob:\n",
    "                    #print(\"label:\",label)\n",
    "                    cnt_prob_total[j]+=1\n",
    "#                     print(\"baseline pred:\",output_baseline_np)\n",
    "#                     print(\"bnm pred:\",output_bnm_np)\n",
    "                    if label_baseline != label_bnm:\n",
    "                        cnt_prob_correct[j]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-18T04:49:21.942791Z",
     "start_time": "2020-06-18T04:49:21.931119Z"
    }
   },
   "outputs": [],
   "source": [
    "cnt_prob_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-18T04:49:31.578533Z",
     "start_time": "2020-06-18T04:49:31.569913Z"
    }
   },
   "outputs": [],
   "source": [
    "#0.5,0.6,0.7,0.8,0.9,0.95,0.97,0.98,0.99\n",
    "cnt_prob_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-18T04:50:15.627206Z",
     "start_time": "2020-06-18T04:50:15.618879Z"
    }
   },
   "outputs": [],
   "source": [
    "pred_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-18T04:50:21.325097Z",
     "start_time": "2020-06-18T04:50:21.319358Z"
    }
   },
   "outputs": [],
   "source": [
    "pre_bnm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T13:29:03.458005Z",
     "start_time": "2020-06-16T13:29:03.453201Z"
    }
   },
   "outputs": [],
   "source": [
    "cnt_06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-18T04:10:21.702033Z",
     "start_time": "2020-06-18T04:10:21.693769Z"
    }
   },
   "outputs": [],
   "source": [
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-18T05:17:48.790841Z",
     "start_time": "2020-06-18T05:17:48.782672Z"
    }
   },
   "outputs": [],
   "source": [
    "correct_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-18T05:17:54.981572Z",
     "start_time": "2020-06-18T05:17:54.974032Z"
    }
   },
   "outputs": [],
   "source": [
    "correct_bnm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T08:34:20.918982Z",
     "start_time": "2020-06-16T08:34:20.910154Z"
    }
   },
   "outputs": [],
   "source": [
    "acc = 0.0\n",
    "for i in range(len(total)):\n",
    "    acc+= correct_baseline[i]/total[i]\n",
    "    print(correct_baseline[i]/total[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T08:33:52.600418Z",
     "start_time": "2020-06-16T08:33:52.595330Z"
    }
   },
   "outputs": [],
   "source": [
    "acc = 0.0\n",
    "for i in range(len(total)):\n",
    "    acc +=correct_bnm[i]/total[i]\n",
    "    print(correct_bnm[i]/total[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T07:43:49.747409Z",
     "start_time": "2020-06-16T07:43:49.738725Z"
    }
   },
   "outputs": [],
   "source": [
    "acc/7.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T07:44:14.530221Z",
     "start_time": "2020-06-16T07:44:14.522556Z"
    }
   },
   "outputs": [],
   "source": [
    "acc/7.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# wcp loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-05T07:38:30.301743Z",
     "start_time": "2020-07-05T07:38:30.288930Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import chainer\n",
    "import chainer.functions as F_chainer\n",
    "from chainer import Variable as V_chain\n",
    "import cupy as xp\n",
    "from cupy.core.dlpack import toDlpack\n",
    "from cupy.core.dlpack import fromDlpack\n",
    "from torch.utils.dlpack import to_dlpack\n",
    "from torch.utils.dlpack import from_dlpack\n",
    "from source.chainer_functions.loss import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-05T07:38:54.975444Z",
     "start_time": "2020-07-05T07:38:54.960314Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from networks import densenet\n",
    "from collections import OrderedDict\n",
    "import torch.utils.model_zoo as model_zoo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## densenet121 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-05T07:44:25.248663Z",
     "start_time": "2020-07-05T07:44:25.198789Z"
    }
   },
   "outputs": [],
   "source": [
    "class DenseNet121MultiScale(nn.Module):\n",
    "    \"\"\"Model modified.\n",
    "    The architecture of our model is the same as standard DenseNet121\n",
    "    except the classifier layer which has an additional sigmoid function.\n",
    "    \"\"\"\n",
    "    def __init__(self, out_size,drop_rate=0):\n",
    "        super(DenseNet121MultiScale, self).__init__()\n",
    "        self.densenet121 = densenet.densenet121(pretrained=True, drop_rate=drop_rate)\n",
    "        num_ftrs = self.densenet121.classifier.in_features\n",
    "        self.densenet121.classifier = nn.Sequential(\n",
    "            nn.Linear(num_ftrs, out_size),\n",
    "            #nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        # Official init from torch repo.\n",
    "        for m in self.densenet121.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        self.drop_rate = drop_rate\n",
    "        self.drop_layer = nn.Dropout(p=drop_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.densenet121.features.conv0(x)\n",
    "        x = self.densenet121.features.norm0(x)\n",
    "        x = self.densenet121.features.relu0(x)\n",
    "        x = self.densenet121.features.pool0(x)\n",
    "        x = self.densenet121.features.denseblock1(x)\n",
    "        fea1 = self.densenet121.features.transition1(x)\n",
    "        fea2 = self.densenet121.features.denseblock2(fea1)\n",
    "        fea2 = self.densenet121.features.transition2(fea2)\n",
    "        fea3 = self.densenet121.features.denseblock3(fea2)\n",
    "        fea3 = self.densenet121.features.transition3(fea3)\n",
    "        print(\"fea1 shape:\",fea1.shape)\n",
    "        print(\"fea2 shape:\",fea2.shape)\n",
    "        print(\"fea3 shape:\",fea3.shape)\n",
    "        features = self.densenet121.features.denseblock4(fea3)\n",
    "        features = self.densenet121.features.norm5(features)\n",
    "        out = F.relu(features, inplace=True) \n",
    "        fea_out2 = F.adaptive_avg_pool2d(fea2, (1, 1)).view(fea2.size(0), -1)\n",
    "        fea_out3 = F.adaptive_avg_pool2d(fea3, (1, 1)).view(fea3.size(0), -1)\n",
    "        out = F.adaptive_avg_pool2d(out, (1, 1)).view(features.size(0), -1)\n",
    "        if self.drop_rate > 0:\n",
    "            out = self.drop_layer(out)\n",
    "        self.activations = out\n",
    "        out = self.densenet121.classifier(out)\n",
    "            \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## wcp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-05T07:53:25.053001Z",
     "start_time": "2020-07-05T07:53:15.413901Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1024, 7, 7])\n",
      "torch.Size([3, 1024])\n",
      "('d weight shape:', torch.Size([6870208]))\n",
      "('d shape:', torch.Size([3, 3, 224, 224]))\n",
      "torch.FloatTensor\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'conv1.weight'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-a2bb3a0258fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwcp_loss_torch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-770d3dcc20af>\u001b[0m in \u001b[0;36mwcp_loss_torch\u001b[0;34m(model, x, logit, epsilon, dr, num_simulations, xi)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;31m#             print(name)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;31m#             break\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mlogit_d\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdelta_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0mlogit_d\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdelta_forward_chainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-02924587f247>\u001b[0m in \u001b[0;36mdelta_forward\u001b[0;34m(model, logit, x, x_d, w_d, size_list)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdelta_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlogit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx_d\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     h = F.relu(F.max_pool2d(F.conv2d(h,model.state_dict()['conv1.weight']+\n\u001b[0m\u001b[1;32m      4\u001b[0m                                      \u001b[0mw_d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'conv1.weight'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                      model.state_dict()['conv1.bias']),2))\n",
      "\u001b[0;31mKeyError\u001b[0m: 'conv1.weight'"
     ]
    }
   ],
   "source": [
    "model = densenet.densenet121(pretrained=True, drop_rate=0)\n",
    "image = torch.rand(3,3,224,224)\n",
    "out = model(image)\n",
    "_,_,output = wcp_loss_torch(model,image,out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-05T09:08:40.415816Z",
     "start_time": "2020-07-05T09:08:37.964218Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('d weight shape:', torch.Size([6870208]))\n",
      "('d shape:', torch.Size([3, 3, 224, 224]))\n",
      "torch.FloatTensor\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "global name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-85ed43690ded>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwcp_loss_torch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-32-9d1bce93cadf>\u001b[0m in \u001b[0;36mwcp_loss_torch\u001b[0;34m(model, x, logit, epsilon, dr, num_simulations, xi)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;31m#             print(name)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;31m#             break\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mlogit_d\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdelta_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0mlogit_d\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdelta_forward_chainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-fdb273133254>\u001b[0m in \u001b[0;36mdelta_forward\u001b[0;34m(model, logit, x, x_d, w_d, size_list)\u001b[0m\n\u001b[1;32m     14\u001b[0m     h = F.conv2d(h,model.state_dict()['features.denseblock1.denselayer1.conv1.weight'] + \n\u001b[1;32m     15\u001b[0m         w_d[size_list[1]:size_list[2]].reshape(model.state_dict()['features.denseblock1.denselayer1.conv1.weight'].shape))\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdensenet121\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdensenet121\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdensenet121\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: global name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "_,_,output = wcp_loss_torch(model,image,out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-05T08:08:09.690929Z",
     "start_time": "2020-07-05T08:08:09.537479Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features.conv0.weight\n",
      "features.norm0.weight\n",
      "features.norm0.bias\n",
      "features.norm0.running_mean\n",
      "features.norm0.running_var\n",
      "features.norm0.num_batches_tracked\n",
      "features.denseblock1.denselayer1.norm1.weight\n",
      "features.denseblock1.denselayer1.norm1.bias\n",
      "features.denseblock1.denselayer1.norm1.running_mean\n",
      "features.denseblock1.denselayer1.norm1.running_var\n",
      "features.denseblock1.denselayer1.norm1.num_batches_tracked\n",
      "features.denseblock1.denselayer1.conv1.weight\n",
      "features.denseblock1.denselayer1.norm2.weight\n",
      "features.denseblock1.denselayer1.norm2.bias\n",
      "features.denseblock1.denselayer1.norm2.running_mean\n",
      "features.denseblock1.denselayer1.norm2.running_var\n",
      "features.denseblock1.denselayer1.norm2.num_batches_tracked\n",
      "features.denseblock1.denselayer1.conv2.weight\n",
      "features.denseblock1.denselayer2.norm1.weight\n",
      "features.denseblock1.denselayer2.norm1.bias\n",
      "features.denseblock1.denselayer2.norm1.running_mean\n",
      "features.denseblock1.denselayer2.norm1.running_var\n",
      "features.denseblock1.denselayer2.norm1.num_batches_tracked\n",
      "features.denseblock1.denselayer2.conv1.weight\n",
      "features.denseblock1.denselayer2.norm2.weight\n",
      "features.denseblock1.denselayer2.norm2.bias\n",
      "features.denseblock1.denselayer2.norm2.running_mean\n",
      "features.denseblock1.denselayer2.norm2.running_var\n",
      "features.denseblock1.denselayer2.norm2.num_batches_tracked\n",
      "features.denseblock1.denselayer2.conv2.weight\n",
      "features.denseblock1.denselayer3.norm1.weight\n",
      "features.denseblock1.denselayer3.norm1.bias\n",
      "features.denseblock1.denselayer3.norm1.running_mean\n",
      "features.denseblock1.denselayer3.norm1.running_var\n",
      "features.denseblock1.denselayer3.norm1.num_batches_tracked\n",
      "features.denseblock1.denselayer3.conv1.weight\n",
      "features.denseblock1.denselayer3.norm2.weight\n",
      "features.denseblock1.denselayer3.norm2.bias\n",
      "features.denseblock1.denselayer3.norm2.running_mean\n",
      "features.denseblock1.denselayer3.norm2.running_var\n",
      "features.denseblock1.denselayer3.norm2.num_batches_tracked\n",
      "features.denseblock1.denselayer3.conv2.weight\n",
      "features.denseblock1.denselayer4.norm1.weight\n",
      "features.denseblock1.denselayer4.norm1.bias\n",
      "features.denseblock1.denselayer4.norm1.running_mean\n",
      "features.denseblock1.denselayer4.norm1.running_var\n",
      "features.denseblock1.denselayer4.norm1.num_batches_tracked\n",
      "features.denseblock1.denselayer4.conv1.weight\n",
      "features.denseblock1.denselayer4.norm2.weight\n",
      "features.denseblock1.denselayer4.norm2.bias\n",
      "features.denseblock1.denselayer4.norm2.running_mean\n",
      "features.denseblock1.denselayer4.norm2.running_var\n",
      "features.denseblock1.denselayer4.norm2.num_batches_tracked\n",
      "features.denseblock1.denselayer4.conv2.weight\n",
      "features.denseblock1.denselayer5.norm1.weight\n",
      "features.denseblock1.denselayer5.norm1.bias\n",
      "features.denseblock1.denselayer5.norm1.running_mean\n",
      "features.denseblock1.denselayer5.norm1.running_var\n",
      "features.denseblock1.denselayer5.norm1.num_batches_tracked\n",
      "features.denseblock1.denselayer5.conv1.weight\n",
      "features.denseblock1.denselayer5.norm2.weight\n",
      "features.denseblock1.denselayer5.norm2.bias\n",
      "features.denseblock1.denselayer5.norm2.running_mean\n",
      "features.denseblock1.denselayer5.norm2.running_var\n",
      "features.denseblock1.denselayer5.norm2.num_batches_tracked\n",
      "features.denseblock1.denselayer5.conv2.weight\n",
      "features.denseblock1.denselayer6.norm1.weight\n",
      "features.denseblock1.denselayer6.norm1.bias\n",
      "features.denseblock1.denselayer6.norm1.running_mean\n",
      "features.denseblock1.denselayer6.norm1.running_var\n",
      "features.denseblock1.denselayer6.norm1.num_batches_tracked\n",
      "features.denseblock1.denselayer6.conv1.weight\n",
      "features.denseblock1.denselayer6.norm2.weight\n",
      "features.denseblock1.denselayer6.norm2.bias\n",
      "features.denseblock1.denselayer6.norm2.running_mean\n",
      "features.denseblock1.denselayer6.norm2.running_var\n",
      "features.denseblock1.denselayer6.norm2.num_batches_tracked\n",
      "features.denseblock1.denselayer6.conv2.weight\n",
      "features.transition1.norm.weight\n",
      "features.transition1.norm.bias\n",
      "features.transition1.norm.running_mean\n",
      "features.transition1.norm.running_var\n",
      "features.transition1.norm.num_batches_tracked\n",
      "features.transition1.conv.weight\n",
      "features.denseblock2.denselayer1.norm1.weight\n",
      "features.denseblock2.denselayer1.norm1.bias\n",
      "features.denseblock2.denselayer1.norm1.running_mean\n",
      "features.denseblock2.denselayer1.norm1.running_var\n",
      "features.denseblock2.denselayer1.norm1.num_batches_tracked\n",
      "features.denseblock2.denselayer1.conv1.weight\n",
      "features.denseblock2.denselayer1.norm2.weight\n",
      "features.denseblock2.denselayer1.norm2.bias\n",
      "features.denseblock2.denselayer1.norm2.running_mean\n",
      "features.denseblock2.denselayer1.norm2.running_var\n",
      "features.denseblock2.denselayer1.norm2.num_batches_tracked\n",
      "features.denseblock2.denselayer1.conv2.weight\n",
      "features.denseblock2.denselayer2.norm1.weight\n",
      "features.denseblock2.denselayer2.norm1.bias\n",
      "features.denseblock2.denselayer2.norm1.running_mean\n",
      "features.denseblock2.denselayer2.norm1.running_var\n",
      "features.denseblock2.denselayer2.norm1.num_batches_tracked\n",
      "features.denseblock2.denselayer2.conv1.weight\n",
      "features.denseblock2.denselayer2.norm2.weight\n",
      "features.denseblock2.denselayer2.norm2.bias\n",
      "features.denseblock2.denselayer2.norm2.running_mean\n",
      "features.denseblock2.denselayer2.norm2.running_var\n",
      "features.denseblock2.denselayer2.norm2.num_batches_tracked\n",
      "features.denseblock2.denselayer2.conv2.weight\n",
      "features.denseblock2.denselayer3.norm1.weight\n",
      "features.denseblock2.denselayer3.norm1.bias\n",
      "features.denseblock2.denselayer3.norm1.running_mean\n",
      "features.denseblock2.denselayer3.norm1.running_var\n",
      "features.denseblock2.denselayer3.norm1.num_batches_tracked\n",
      "features.denseblock2.denselayer3.conv1.weight\n",
      "features.denseblock2.denselayer3.norm2.weight\n",
      "features.denseblock2.denselayer3.norm2.bias\n",
      "features.denseblock2.denselayer3.norm2.running_mean\n",
      "features.denseblock2.denselayer3.norm2.running_var\n",
      "features.denseblock2.denselayer3.norm2.num_batches_tracked\n",
      "features.denseblock2.denselayer3.conv2.weight\n",
      "features.denseblock2.denselayer4.norm1.weight\n",
      "features.denseblock2.denselayer4.norm1.bias\n",
      "features.denseblock2.denselayer4.norm1.running_mean\n",
      "features.denseblock2.denselayer4.norm1.running_var\n",
      "features.denseblock2.denselayer4.norm1.num_batches_tracked\n",
      "features.denseblock2.denselayer4.conv1.weight\n",
      "features.denseblock2.denselayer4.norm2.weight\n",
      "features.denseblock2.denselayer4.norm2.bias\n",
      "features.denseblock2.denselayer4.norm2.running_mean\n",
      "features.denseblock2.denselayer4.norm2.running_var\n",
      "features.denseblock2.denselayer4.norm2.num_batches_tracked\n",
      "features.denseblock2.denselayer4.conv2.weight\n",
      "features.denseblock2.denselayer5.norm1.weight\n",
      "features.denseblock2.denselayer5.norm1.bias\n",
      "features.denseblock2.denselayer5.norm1.running_mean\n",
      "features.denseblock2.denselayer5.norm1.running_var\n",
      "features.denseblock2.denselayer5.norm1.num_batches_tracked\n",
      "features.denseblock2.denselayer5.conv1.weight\n",
      "features.denseblock2.denselayer5.norm2.weight\n",
      "features.denseblock2.denselayer5.norm2.bias\n",
      "features.denseblock2.denselayer5.norm2.running_mean\n",
      "features.denseblock2.denselayer5.norm2.running_var\n",
      "features.denseblock2.denselayer5.norm2.num_batches_tracked\n",
      "features.denseblock2.denselayer5.conv2.weight\n",
      "features.denseblock2.denselayer6.norm1.weight\n",
      "features.denseblock2.denselayer6.norm1.bias\n",
      "features.denseblock2.denselayer6.norm1.running_mean\n",
      "features.denseblock2.denselayer6.norm1.running_var\n",
      "features.denseblock2.denselayer6.norm1.num_batches_tracked\n",
      "features.denseblock2.denselayer6.conv1.weight\n",
      "features.denseblock2.denselayer6.norm2.weight\n",
      "features.denseblock2.denselayer6.norm2.bias\n",
      "features.denseblock2.denselayer6.norm2.running_mean\n",
      "features.denseblock2.denselayer6.norm2.running_var\n",
      "features.denseblock2.denselayer6.norm2.num_batches_tracked\n",
      "features.denseblock2.denselayer6.conv2.weight\n",
      "features.denseblock2.denselayer7.norm1.weight\n",
      "features.denseblock2.denselayer7.norm1.bias\n",
      "features.denseblock2.denselayer7.norm1.running_mean\n",
      "features.denseblock2.denselayer7.norm1.running_var\n",
      "features.denseblock2.denselayer7.norm1.num_batches_tracked\n",
      "features.denseblock2.denselayer7.conv1.weight\n",
      "features.denseblock2.denselayer7.norm2.weight\n",
      "features.denseblock2.denselayer7.norm2.bias\n",
      "features.denseblock2.denselayer7.norm2.running_mean\n",
      "features.denseblock2.denselayer7.norm2.running_var\n",
      "features.denseblock2.denselayer7.norm2.num_batches_tracked\n",
      "features.denseblock2.denselayer7.conv2.weight\n",
      "features.denseblock2.denselayer8.norm1.weight\n",
      "features.denseblock2.denselayer8.norm1.bias\n",
      "features.denseblock2.denselayer8.norm1.running_mean\n",
      "features.denseblock2.denselayer8.norm1.running_var\n",
      "features.denseblock2.denselayer8.norm1.num_batches_tracked\n",
      "features.denseblock2.denselayer8.conv1.weight\n",
      "features.denseblock2.denselayer8.norm2.weight\n",
      "features.denseblock2.denselayer8.norm2.bias\n",
      "features.denseblock2.denselayer8.norm2.running_mean\n",
      "features.denseblock2.denselayer8.norm2.running_var\n",
      "features.denseblock2.denselayer8.norm2.num_batches_tracked\n",
      "features.denseblock2.denselayer8.conv2.weight\n",
      "features.denseblock2.denselayer9.norm1.weight\n",
      "features.denseblock2.denselayer9.norm1.bias\n",
      "features.denseblock2.denselayer9.norm1.running_mean\n",
      "features.denseblock2.denselayer9.norm1.running_var\n",
      "features.denseblock2.denselayer9.norm1.num_batches_tracked\n",
      "features.denseblock2.denselayer9.conv1.weight\n",
      "features.denseblock2.denselayer9.norm2.weight\n",
      "features.denseblock2.denselayer9.norm2.bias\n",
      "features.denseblock2.denselayer9.norm2.running_mean\n",
      "features.denseblock2.denselayer9.norm2.running_var\n",
      "features.denseblock2.denselayer9.norm2.num_batches_tracked\n",
      "features.denseblock2.denselayer9.conv2.weight\n",
      "features.denseblock2.denselayer10.norm1.weight\n",
      "features.denseblock2.denselayer10.norm1.bias\n",
      "features.denseblock2.denselayer10.norm1.running_mean\n",
      "features.denseblock2.denselayer10.norm1.running_var\n",
      "features.denseblock2.denselayer10.norm1.num_batches_tracked\n",
      "features.denseblock2.denselayer10.conv1.weight\n",
      "features.denseblock2.denselayer10.norm2.weight\n",
      "features.denseblock2.denselayer10.norm2.bias\n",
      "features.denseblock2.denselayer10.norm2.running_mean\n",
      "features.denseblock2.denselayer10.norm2.running_var\n",
      "features.denseblock2.denselayer10.norm2.num_batches_tracked\n",
      "features.denseblock2.denselayer10.conv2.weight\n",
      "features.denseblock2.denselayer11.norm1.weight\n",
      "features.denseblock2.denselayer11.norm1.bias\n",
      "features.denseblock2.denselayer11.norm1.running_mean\n",
      "features.denseblock2.denselayer11.norm1.running_var\n",
      "features.denseblock2.denselayer11.norm1.num_batches_tracked\n",
      "features.denseblock2.denselayer11.conv1.weight\n",
      "features.denseblock2.denselayer11.norm2.weight\n",
      "features.denseblock2.denselayer11.norm2.bias\n",
      "features.denseblock2.denselayer11.norm2.running_mean\n",
      "features.denseblock2.denselayer11.norm2.running_var\n",
      "features.denseblock2.denselayer11.norm2.num_batches_tracked\n",
      "features.denseblock2.denselayer11.conv2.weight\n",
      "features.denseblock2.denselayer12.norm1.weight\n",
      "features.denseblock2.denselayer12.norm1.bias\n",
      "features.denseblock2.denselayer12.norm1.running_mean\n",
      "features.denseblock2.denselayer12.norm1.running_var\n",
      "features.denseblock2.denselayer12.norm1.num_batches_tracked\n",
      "features.denseblock2.denselayer12.conv1.weight\n",
      "features.denseblock2.denselayer12.norm2.weight\n",
      "features.denseblock2.denselayer12.norm2.bias\n",
      "features.denseblock2.denselayer12.norm2.running_mean\n",
      "features.denseblock2.denselayer12.norm2.running_var\n",
      "features.denseblock2.denselayer12.norm2.num_batches_tracked\n",
      "features.denseblock2.denselayer12.conv2.weight\n",
      "features.transition2.norm.weight\n",
      "features.transition2.norm.bias\n",
      "features.transition2.norm.running_mean\n",
      "features.transition2.norm.running_var\n",
      "features.transition2.norm.num_batches_tracked\n",
      "features.transition2.conv.weight\n",
      "features.denseblock3.denselayer1.norm1.weight\n",
      "features.denseblock3.denselayer1.norm1.bias\n",
      "features.denseblock3.denselayer1.norm1.running_mean\n",
      "features.denseblock3.denselayer1.norm1.running_var\n",
      "features.denseblock3.denselayer1.norm1.num_batches_tracked\n",
      "features.denseblock3.denselayer1.conv1.weight\n",
      "features.denseblock3.denselayer1.norm2.weight\n",
      "features.denseblock3.denselayer1.norm2.bias\n",
      "features.denseblock3.denselayer1.norm2.running_mean\n",
      "features.denseblock3.denselayer1.norm2.running_var\n",
      "features.denseblock3.denselayer1.norm2.num_batches_tracked\n",
      "features.denseblock3.denselayer1.conv2.weight\n",
      "features.denseblock3.denselayer2.norm1.weight\n",
      "features.denseblock3.denselayer2.norm1.bias\n",
      "features.denseblock3.denselayer2.norm1.running_mean\n",
      "features.denseblock3.denselayer2.norm1.running_var\n",
      "features.denseblock3.denselayer2.norm1.num_batches_tracked\n",
      "features.denseblock3.denselayer2.conv1.weight\n",
      "features.denseblock3.denselayer2.norm2.weight\n",
      "features.denseblock3.denselayer2.norm2.bias\n",
      "features.denseblock3.denselayer2.norm2.running_mean\n",
      "features.denseblock3.denselayer2.norm2.running_var\n",
      "features.denseblock3.denselayer2.norm2.num_batches_tracked\n",
      "features.denseblock3.denselayer2.conv2.weight\n",
      "features.denseblock3.denselayer3.norm1.weight\n",
      "features.denseblock3.denselayer3.norm1.bias\n",
      "features.denseblock3.denselayer3.norm1.running_mean\n",
      "features.denseblock3.denselayer3.norm1.running_var\n",
      "features.denseblock3.denselayer3.norm1.num_batches_tracked\n",
      "features.denseblock3.denselayer3.conv1.weight\n",
      "features.denseblock3.denselayer3.norm2.weight\n",
      "features.denseblock3.denselayer3.norm2.bias\n",
      "features.denseblock3.denselayer3.norm2.running_mean\n",
      "features.denseblock3.denselayer3.norm2.running_var\n",
      "features.denseblock3.denselayer3.norm2.num_batches_tracked\n",
      "features.denseblock3.denselayer3.conv2.weight\n",
      "features.denseblock3.denselayer4.norm1.weight\n",
      "features.denseblock3.denselayer4.norm1.bias\n",
      "features.denseblock3.denselayer4.norm1.running_mean\n",
      "features.denseblock3.denselayer4.norm1.running_var\n",
      "features.denseblock3.denselayer4.norm1.num_batches_tracked\n",
      "features.denseblock3.denselayer4.conv1.weight\n",
      "features.denseblock3.denselayer4.norm2.weight\n",
      "features.denseblock3.denselayer4.norm2.bias\n",
      "features.denseblock3.denselayer4.norm2.running_mean\n",
      "features.denseblock3.denselayer4.norm2.running_var\n",
      "features.denseblock3.denselayer4.norm2.num_batches_tracked\n",
      "features.denseblock3.denselayer4.conv2.weight\n",
      "features.denseblock3.denselayer5.norm1.weight\n",
      "features.denseblock3.denselayer5.norm1.bias\n",
      "features.denseblock3.denselayer5.norm1.running_mean\n",
      "features.denseblock3.denselayer5.norm1.running_var\n",
      "features.denseblock3.denselayer5.norm1.num_batches_tracked\n",
      "features.denseblock3.denselayer5.conv1.weight\n",
      "features.denseblock3.denselayer5.norm2.weight\n",
      "features.denseblock3.denselayer5.norm2.bias\n",
      "features.denseblock3.denselayer5.norm2.running_mean\n",
      "features.denseblock3.denselayer5.norm2.running_var\n",
      "features.denseblock3.denselayer5.norm2.num_batches_tracked\n",
      "features.denseblock3.denselayer5.conv2.weight\n",
      "features.denseblock3.denselayer6.norm1.weight\n",
      "features.denseblock3.denselayer6.norm1.bias\n",
      "features.denseblock3.denselayer6.norm1.running_mean\n",
      "features.denseblock3.denselayer6.norm1.running_var\n",
      "features.denseblock3.denselayer6.norm1.num_batches_tracked\n",
      "features.denseblock3.denselayer6.conv1.weight\n",
      "features.denseblock3.denselayer6.norm2.weight\n",
      "features.denseblock3.denselayer6.norm2.bias\n",
      "features.denseblock3.denselayer6.norm2.running_mean\n",
      "features.denseblock3.denselayer6.norm2.running_var\n",
      "features.denseblock3.denselayer6.norm2.num_batches_tracked\n",
      "features.denseblock3.denselayer6.conv2.weight\n",
      "features.denseblock3.denselayer7.norm1.weight\n",
      "features.denseblock3.denselayer7.norm1.bias\n",
      "features.denseblock3.denselayer7.norm1.running_mean\n",
      "features.denseblock3.denselayer7.norm1.running_var\n",
      "features.denseblock3.denselayer7.norm1.num_batches_tracked\n",
      "features.denseblock3.denselayer7.conv1.weight\n",
      "features.denseblock3.denselayer7.norm2.weight\n",
      "features.denseblock3.denselayer7.norm2.bias\n",
      "features.denseblock3.denselayer7.norm2.running_mean\n",
      "features.denseblock3.denselayer7.norm2.running_var\n",
      "features.denseblock3.denselayer7.norm2.num_batches_tracked\n",
      "features.denseblock3.denselayer7.conv2.weight\n",
      "features.denseblock3.denselayer8.norm1.weight\n",
      "features.denseblock3.denselayer8.norm1.bias\n",
      "features.denseblock3.denselayer8.norm1.running_mean\n",
      "features.denseblock3.denselayer8.norm1.running_var\n",
      "features.denseblock3.denselayer8.norm1.num_batches_tracked\n",
      "features.denseblock3.denselayer8.conv1.weight\n",
      "features.denseblock3.denselayer8.norm2.weight\n",
      "features.denseblock3.denselayer8.norm2.bias\n",
      "features.denseblock3.denselayer8.norm2.running_mean\n",
      "features.denseblock3.denselayer8.norm2.running_var\n",
      "features.denseblock3.denselayer8.norm2.num_batches_tracked\n",
      "features.denseblock3.denselayer8.conv2.weight\n",
      "features.denseblock3.denselayer9.norm1.weight\n",
      "features.denseblock3.denselayer9.norm1.bias\n",
      "features.denseblock3.denselayer9.norm1.running_mean\n",
      "features.denseblock3.denselayer9.norm1.running_var\n",
      "features.denseblock3.denselayer9.norm1.num_batches_tracked\n",
      "features.denseblock3.denselayer9.conv1.weight\n",
      "features.denseblock3.denselayer9.norm2.weight\n",
      "features.denseblock3.denselayer9.norm2.bias\n",
      "features.denseblock3.denselayer9.norm2.running_mean\n",
      "features.denseblock3.denselayer9.norm2.running_var\n",
      "features.denseblock3.denselayer9.norm2.num_batches_tracked\n",
      "features.denseblock3.denselayer9.conv2.weight\n",
      "features.denseblock3.denselayer10.norm1.weight\n",
      "features.denseblock3.denselayer10.norm1.bias\n",
      "features.denseblock3.denselayer10.norm1.running_mean\n",
      "features.denseblock3.denselayer10.norm1.running_var\n",
      "features.denseblock3.denselayer10.norm1.num_batches_tracked\n",
      "features.denseblock3.denselayer10.conv1.weight\n",
      "features.denseblock3.denselayer10.norm2.weight\n",
      "features.denseblock3.denselayer10.norm2.bias\n",
      "features.denseblock3.denselayer10.norm2.running_mean\n",
      "features.denseblock3.denselayer10.norm2.running_var\n",
      "features.denseblock3.denselayer10.norm2.num_batches_tracked\n",
      "features.denseblock3.denselayer10.conv2.weight\n",
      "features.denseblock3.denselayer11.norm1.weight\n",
      "features.denseblock3.denselayer11.norm1.bias\n",
      "features.denseblock3.denselayer11.norm1.running_mean\n",
      "features.denseblock3.denselayer11.norm1.running_var\n",
      "features.denseblock3.denselayer11.norm1.num_batches_tracked\n",
      "features.denseblock3.denselayer11.conv1.weight\n",
      "features.denseblock3.denselayer11.norm2.weight\n",
      "features.denseblock3.denselayer11.norm2.bias\n",
      "features.denseblock3.denselayer11.norm2.running_mean\n",
      "features.denseblock3.denselayer11.norm2.running_var\n",
      "features.denseblock3.denselayer11.norm2.num_batches_tracked\n",
      "features.denseblock3.denselayer11.conv2.weight\n",
      "features.denseblock3.denselayer12.norm1.weight\n",
      "features.denseblock3.denselayer12.norm1.bias\n",
      "features.denseblock3.denselayer12.norm1.running_mean\n",
      "features.denseblock3.denselayer12.norm1.running_var\n",
      "features.denseblock3.denselayer12.norm1.num_batches_tracked\n",
      "features.denseblock3.denselayer12.conv1.weight\n",
      "features.denseblock3.denselayer12.norm2.weight\n",
      "features.denseblock3.denselayer12.norm2.bias\n",
      "features.denseblock3.denselayer12.norm2.running_mean\n",
      "features.denseblock3.denselayer12.norm2.running_var\n",
      "features.denseblock3.denselayer12.norm2.num_batches_tracked\n",
      "features.denseblock3.denselayer12.conv2.weight\n",
      "features.denseblock3.denselayer13.norm1.weight\n",
      "features.denseblock3.denselayer13.norm1.bias\n",
      "features.denseblock3.denselayer13.norm1.running_mean\n",
      "features.denseblock3.denselayer13.norm1.running_var\n",
      "features.denseblock3.denselayer13.norm1.num_batches_tracked\n",
      "features.denseblock3.denselayer13.conv1.weight\n",
      "features.denseblock3.denselayer13.norm2.weight\n",
      "features.denseblock3.denselayer13.norm2.bias\n",
      "features.denseblock3.denselayer13.norm2.running_mean\n",
      "features.denseblock3.denselayer13.norm2.running_var\n",
      "features.denseblock3.denselayer13.norm2.num_batches_tracked\n",
      "features.denseblock3.denselayer13.conv2.weight\n",
      "features.denseblock3.denselayer14.norm1.weight\n",
      "features.denseblock3.denselayer14.norm1.bias\n",
      "features.denseblock3.denselayer14.norm1.running_mean\n",
      "features.denseblock3.denselayer14.norm1.running_var\n",
      "features.denseblock3.denselayer14.norm1.num_batches_tracked\n",
      "features.denseblock3.denselayer14.conv1.weight\n",
      "features.denseblock3.denselayer14.norm2.weight\n",
      "features.denseblock3.denselayer14.norm2.bias\n",
      "features.denseblock3.denselayer14.norm2.running_mean\n",
      "features.denseblock3.denselayer14.norm2.running_var\n",
      "features.denseblock3.denselayer14.norm2.num_batches_tracked\n",
      "features.denseblock3.denselayer14.conv2.weight\n",
      "features.denseblock3.denselayer15.norm1.weight\n",
      "features.denseblock3.denselayer15.norm1.bias\n",
      "features.denseblock3.denselayer15.norm1.running_mean\n",
      "features.denseblock3.denselayer15.norm1.running_var\n",
      "features.denseblock3.denselayer15.norm1.num_batches_tracked\n",
      "features.denseblock3.denselayer15.conv1.weight\n",
      "features.denseblock3.denselayer15.norm2.weight\n",
      "features.denseblock3.denselayer15.norm2.bias\n",
      "features.denseblock3.denselayer15.norm2.running_mean\n",
      "features.denseblock3.denselayer15.norm2.running_var\n",
      "features.denseblock3.denselayer15.norm2.num_batches_tracked\n",
      "features.denseblock3.denselayer15.conv2.weight\n",
      "features.denseblock3.denselayer16.norm1.weight\n",
      "features.denseblock3.denselayer16.norm1.bias\n",
      "features.denseblock3.denselayer16.norm1.running_mean\n",
      "features.denseblock3.denselayer16.norm1.running_var\n",
      "features.denseblock3.denselayer16.norm1.num_batches_tracked\n",
      "features.denseblock3.denselayer16.conv1.weight\n",
      "features.denseblock3.denselayer16.norm2.weight\n",
      "features.denseblock3.denselayer16.norm2.bias\n",
      "features.denseblock3.denselayer16.norm2.running_mean\n",
      "features.denseblock3.denselayer16.norm2.running_var\n",
      "features.denseblock3.denselayer16.norm2.num_batches_tracked\n",
      "features.denseblock3.denselayer16.conv2.weight\n",
      "features.denseblock3.denselayer17.norm1.weight\n",
      "features.denseblock3.denselayer17.norm1.bias\n",
      "features.denseblock3.denselayer17.norm1.running_mean\n",
      "features.denseblock3.denselayer17.norm1.running_var\n",
      "features.denseblock3.denselayer17.norm1.num_batches_tracked\n",
      "features.denseblock3.denselayer17.conv1.weight\n",
      "features.denseblock3.denselayer17.norm2.weight\n",
      "features.denseblock3.denselayer17.norm2.bias\n",
      "features.denseblock3.denselayer17.norm2.running_mean\n",
      "features.denseblock3.denselayer17.norm2.running_var\n",
      "features.denseblock3.denselayer17.norm2.num_batches_tracked\n",
      "features.denseblock3.denselayer17.conv2.weight\n",
      "features.denseblock3.denselayer18.norm1.weight\n",
      "features.denseblock3.denselayer18.norm1.bias\n",
      "features.denseblock3.denselayer18.norm1.running_mean\n",
      "features.denseblock3.denselayer18.norm1.running_var\n",
      "features.denseblock3.denselayer18.norm1.num_batches_tracked\n",
      "features.denseblock3.denselayer18.conv1.weight\n",
      "features.denseblock3.denselayer18.norm2.weight\n",
      "features.denseblock3.denselayer18.norm2.bias\n",
      "features.denseblock3.denselayer18.norm2.running_mean\n",
      "features.denseblock3.denselayer18.norm2.running_var\n",
      "features.denseblock3.denselayer18.norm2.num_batches_tracked\n",
      "features.denseblock3.denselayer18.conv2.weight\n",
      "features.denseblock3.denselayer19.norm1.weight\n",
      "features.denseblock3.denselayer19.norm1.bias\n",
      "features.denseblock3.denselayer19.norm1.running_mean\n",
      "features.denseblock3.denselayer19.norm1.running_var\n",
      "features.denseblock3.denselayer19.norm1.num_batches_tracked\n",
      "features.denseblock3.denselayer19.conv1.weight\n",
      "features.denseblock3.denselayer19.norm2.weight\n",
      "features.denseblock3.denselayer19.norm2.bias\n",
      "features.denseblock3.denselayer19.norm2.running_mean\n",
      "features.denseblock3.denselayer19.norm2.running_var\n",
      "features.denseblock3.denselayer19.norm2.num_batches_tracked\n",
      "features.denseblock3.denselayer19.conv2.weight\n",
      "features.denseblock3.denselayer20.norm1.weight\n",
      "features.denseblock3.denselayer20.norm1.bias\n",
      "features.denseblock3.denselayer20.norm1.running_mean\n",
      "features.denseblock3.denselayer20.norm1.running_var\n",
      "features.denseblock3.denselayer20.norm1.num_batches_tracked\n",
      "features.denseblock3.denselayer20.conv1.weight\n",
      "features.denseblock3.denselayer20.norm2.weight\n",
      "features.denseblock3.denselayer20.norm2.bias\n",
      "features.denseblock3.denselayer20.norm2.running_mean\n",
      "features.denseblock3.denselayer20.norm2.running_var\n",
      "features.denseblock3.denselayer20.norm2.num_batches_tracked\n",
      "features.denseblock3.denselayer20.conv2.weight\n",
      "features.denseblock3.denselayer21.norm1.weight\n",
      "features.denseblock3.denselayer21.norm1.bias\n",
      "features.denseblock3.denselayer21.norm1.running_mean\n",
      "features.denseblock3.denselayer21.norm1.running_var\n",
      "features.denseblock3.denselayer21.norm1.num_batches_tracked\n",
      "features.denseblock3.denselayer21.conv1.weight\n",
      "features.denseblock3.denselayer21.norm2.weight\n",
      "features.denseblock3.denselayer21.norm2.bias\n",
      "features.denseblock3.denselayer21.norm2.running_mean\n",
      "features.denseblock3.denselayer21.norm2.running_var\n",
      "features.denseblock3.denselayer21.norm2.num_batches_tracked\n",
      "features.denseblock3.denselayer21.conv2.weight\n",
      "features.denseblock3.denselayer22.norm1.weight\n",
      "features.denseblock3.denselayer22.norm1.bias\n",
      "features.denseblock3.denselayer22.norm1.running_mean\n",
      "features.denseblock3.denselayer22.norm1.running_var\n",
      "features.denseblock3.denselayer22.norm1.num_batches_tracked\n",
      "features.denseblock3.denselayer22.conv1.weight\n",
      "features.denseblock3.denselayer22.norm2.weight\n",
      "features.denseblock3.denselayer22.norm2.bias\n",
      "features.denseblock3.denselayer22.norm2.running_mean\n",
      "features.denseblock3.denselayer22.norm2.running_var\n",
      "features.denseblock3.denselayer22.norm2.num_batches_tracked\n",
      "features.denseblock3.denselayer22.conv2.weight\n",
      "features.denseblock3.denselayer23.norm1.weight\n",
      "features.denseblock3.denselayer23.norm1.bias\n",
      "features.denseblock3.denselayer23.norm1.running_mean\n",
      "features.denseblock3.denselayer23.norm1.running_var\n",
      "features.denseblock3.denselayer23.norm1.num_batches_tracked\n",
      "features.denseblock3.denselayer23.conv1.weight\n",
      "features.denseblock3.denselayer23.norm2.weight\n",
      "features.denseblock3.denselayer23.norm2.bias\n",
      "features.denseblock3.denselayer23.norm2.running_mean\n",
      "features.denseblock3.denselayer23.norm2.running_var\n",
      "features.denseblock3.denselayer23.norm2.num_batches_tracked\n",
      "features.denseblock3.denselayer23.conv2.weight\n",
      "features.denseblock3.denselayer24.norm1.weight\n",
      "features.denseblock3.denselayer24.norm1.bias\n",
      "features.denseblock3.denselayer24.norm1.running_mean\n",
      "features.denseblock3.denselayer24.norm1.running_var\n",
      "features.denseblock3.denselayer24.norm1.num_batches_tracked\n",
      "features.denseblock3.denselayer24.conv1.weight\n",
      "features.denseblock3.denselayer24.norm2.weight\n",
      "features.denseblock3.denselayer24.norm2.bias\n",
      "features.denseblock3.denselayer24.norm2.running_mean\n",
      "features.denseblock3.denselayer24.norm2.running_var\n",
      "features.denseblock3.denselayer24.norm2.num_batches_tracked\n",
      "features.denseblock3.denselayer24.conv2.weight\n",
      "features.transition3.norm.weight\n",
      "features.transition3.norm.bias\n",
      "features.transition3.norm.running_mean\n",
      "features.transition3.norm.running_var\n",
      "features.transition3.norm.num_batches_tracked\n",
      "features.transition3.conv.weight\n",
      "features.denseblock4.denselayer1.norm1.weight\n",
      "features.denseblock4.denselayer1.norm1.bias\n",
      "features.denseblock4.denselayer1.norm1.running_mean\n",
      "features.denseblock4.denselayer1.norm1.running_var\n",
      "features.denseblock4.denselayer1.norm1.num_batches_tracked\n",
      "features.denseblock4.denselayer1.conv1.weight\n",
      "features.denseblock4.denselayer1.norm2.weight\n",
      "features.denseblock4.denselayer1.norm2.bias\n",
      "features.denseblock4.denselayer1.norm2.running_mean\n",
      "features.denseblock4.denselayer1.norm2.running_var\n",
      "features.denseblock4.denselayer1.norm2.num_batches_tracked\n",
      "features.denseblock4.denselayer1.conv2.weight\n",
      "features.denseblock4.denselayer2.norm1.weight\n",
      "features.denseblock4.denselayer2.norm1.bias\n",
      "features.denseblock4.denselayer2.norm1.running_mean\n",
      "features.denseblock4.denselayer2.norm1.running_var\n",
      "features.denseblock4.denselayer2.norm1.num_batches_tracked\n",
      "features.denseblock4.denselayer2.conv1.weight\n",
      "features.denseblock4.denselayer2.norm2.weight\n",
      "features.denseblock4.denselayer2.norm2.bias\n",
      "features.denseblock4.denselayer2.norm2.running_mean\n",
      "features.denseblock4.denselayer2.norm2.running_var\n",
      "features.denseblock4.denselayer2.norm2.num_batches_tracked\n",
      "features.denseblock4.denselayer2.conv2.weight\n",
      "features.denseblock4.denselayer3.norm1.weight\n",
      "features.denseblock4.denselayer3.norm1.bias\n",
      "features.denseblock4.denselayer3.norm1.running_mean\n",
      "features.denseblock4.denselayer3.norm1.running_var\n",
      "features.denseblock4.denselayer3.norm1.num_batches_tracked\n",
      "features.denseblock4.denselayer3.conv1.weight\n",
      "features.denseblock4.denselayer3.norm2.weight\n",
      "features.denseblock4.denselayer3.norm2.bias\n",
      "features.denseblock4.denselayer3.norm2.running_mean\n",
      "features.denseblock4.denselayer3.norm2.running_var\n",
      "features.denseblock4.denselayer3.norm2.num_batches_tracked\n",
      "features.denseblock4.denselayer3.conv2.weight\n",
      "features.denseblock4.denselayer4.norm1.weight\n",
      "features.denseblock4.denselayer4.norm1.bias\n",
      "features.denseblock4.denselayer4.norm1.running_mean\n",
      "features.denseblock4.denselayer4.norm1.running_var\n",
      "features.denseblock4.denselayer4.norm1.num_batches_tracked\n",
      "features.denseblock4.denselayer4.conv1.weight\n",
      "features.denseblock4.denselayer4.norm2.weight\n",
      "features.denseblock4.denselayer4.norm2.bias\n",
      "features.denseblock4.denselayer4.norm2.running_mean\n",
      "features.denseblock4.denselayer4.norm2.running_var\n",
      "features.denseblock4.denselayer4.norm2.num_batches_tracked\n",
      "features.denseblock4.denselayer4.conv2.weight\n",
      "features.denseblock4.denselayer5.norm1.weight\n",
      "features.denseblock4.denselayer5.norm1.bias\n",
      "features.denseblock4.denselayer5.norm1.running_mean\n",
      "features.denseblock4.denselayer5.norm1.running_var\n",
      "features.denseblock4.denselayer5.norm1.num_batches_tracked\n",
      "features.denseblock4.denselayer5.conv1.weight\n",
      "features.denseblock4.denselayer5.norm2.weight\n",
      "features.denseblock4.denselayer5.norm2.bias\n",
      "features.denseblock4.denselayer5.norm2.running_mean\n",
      "features.denseblock4.denselayer5.norm2.running_var\n",
      "features.denseblock4.denselayer5.norm2.num_batches_tracked\n",
      "features.denseblock4.denselayer5.conv2.weight\n",
      "features.denseblock4.denselayer6.norm1.weight\n",
      "features.denseblock4.denselayer6.norm1.bias\n",
      "features.denseblock4.denselayer6.norm1.running_mean\n",
      "features.denseblock4.denselayer6.norm1.running_var\n",
      "features.denseblock4.denselayer6.norm1.num_batches_tracked\n",
      "features.denseblock4.denselayer6.conv1.weight\n",
      "features.denseblock4.denselayer6.norm2.weight\n",
      "features.denseblock4.denselayer6.norm2.bias\n",
      "features.denseblock4.denselayer6.norm2.running_mean\n",
      "features.denseblock4.denselayer6.norm2.running_var\n",
      "features.denseblock4.denselayer6.norm2.num_batches_tracked\n",
      "features.denseblock4.denselayer6.conv2.weight\n",
      "features.denseblock4.denselayer7.norm1.weight\n",
      "features.denseblock4.denselayer7.norm1.bias\n",
      "features.denseblock4.denselayer7.norm1.running_mean\n",
      "features.denseblock4.denselayer7.norm1.running_var\n",
      "features.denseblock4.denselayer7.norm1.num_batches_tracked\n",
      "features.denseblock4.denselayer7.conv1.weight\n",
      "features.denseblock4.denselayer7.norm2.weight\n",
      "features.denseblock4.denselayer7.norm2.bias\n",
      "features.denseblock4.denselayer7.norm2.running_mean\n",
      "features.denseblock4.denselayer7.norm2.running_var\n",
      "features.denseblock4.denselayer7.norm2.num_batches_tracked\n",
      "features.denseblock4.denselayer7.conv2.weight\n",
      "features.denseblock4.denselayer8.norm1.weight\n",
      "features.denseblock4.denselayer8.norm1.bias\n",
      "features.denseblock4.denselayer8.norm1.running_mean\n",
      "features.denseblock4.denselayer8.norm1.running_var\n",
      "features.denseblock4.denselayer8.norm1.num_batches_tracked\n",
      "features.denseblock4.denselayer8.conv1.weight\n",
      "features.denseblock4.denselayer8.norm2.weight\n",
      "features.denseblock4.denselayer8.norm2.bias\n",
      "features.denseblock4.denselayer8.norm2.running_mean\n",
      "features.denseblock4.denselayer8.norm2.running_var\n",
      "features.denseblock4.denselayer8.norm2.num_batches_tracked\n",
      "features.denseblock4.denselayer8.conv2.weight\n",
      "features.denseblock4.denselayer9.norm1.weight\n",
      "features.denseblock4.denselayer9.norm1.bias\n",
      "features.denseblock4.denselayer9.norm1.running_mean\n",
      "features.denseblock4.denselayer9.norm1.running_var\n",
      "features.denseblock4.denselayer9.norm1.num_batches_tracked\n",
      "features.denseblock4.denselayer9.conv1.weight\n",
      "features.denseblock4.denselayer9.norm2.weight\n",
      "features.denseblock4.denselayer9.norm2.bias\n",
      "features.denseblock4.denselayer9.norm2.running_mean\n",
      "features.denseblock4.denselayer9.norm2.running_var\n",
      "features.denseblock4.denselayer9.norm2.num_batches_tracked\n",
      "features.denseblock4.denselayer9.conv2.weight\n",
      "features.denseblock4.denselayer10.norm1.weight\n",
      "features.denseblock4.denselayer10.norm1.bias\n",
      "features.denseblock4.denselayer10.norm1.running_mean\n",
      "features.denseblock4.denselayer10.norm1.running_var\n",
      "features.denseblock4.denselayer10.norm1.num_batches_tracked\n",
      "features.denseblock4.denselayer10.conv1.weight\n",
      "features.denseblock4.denselayer10.norm2.weight\n",
      "features.denseblock4.denselayer10.norm2.bias\n",
      "features.denseblock4.denselayer10.norm2.running_mean\n",
      "features.denseblock4.denselayer10.norm2.running_var\n",
      "features.denseblock4.denselayer10.norm2.num_batches_tracked\n",
      "features.denseblock4.denselayer10.conv2.weight\n",
      "features.denseblock4.denselayer11.norm1.weight\n",
      "features.denseblock4.denselayer11.norm1.bias\n",
      "features.denseblock4.denselayer11.norm1.running_mean\n",
      "features.denseblock4.denselayer11.norm1.running_var\n",
      "features.denseblock4.denselayer11.norm1.num_batches_tracked\n",
      "features.denseblock4.denselayer11.conv1.weight\n",
      "features.denseblock4.denselayer11.norm2.weight\n",
      "features.denseblock4.denselayer11.norm2.bias\n",
      "features.denseblock4.denselayer11.norm2.running_mean\n",
      "features.denseblock4.denselayer11.norm2.running_var\n",
      "features.denseblock4.denselayer11.norm2.num_batches_tracked\n",
      "features.denseblock4.denselayer11.conv2.weight\n",
      "features.denseblock4.denselayer12.norm1.weight\n",
      "features.denseblock4.denselayer12.norm1.bias\n",
      "features.denseblock4.denselayer12.norm1.running_mean\n",
      "features.denseblock4.denselayer12.norm1.running_var\n",
      "features.denseblock4.denselayer12.norm1.num_batches_tracked\n",
      "features.denseblock4.denselayer12.conv1.weight\n",
      "features.denseblock4.denselayer12.norm2.weight\n",
      "features.denseblock4.denselayer12.norm2.bias\n",
      "features.denseblock4.denselayer12.norm2.running_mean\n",
      "features.denseblock4.denselayer12.norm2.running_var\n",
      "features.denseblock4.denselayer12.norm2.num_batches_tracked\n",
      "features.denseblock4.denselayer12.conv2.weight\n",
      "features.denseblock4.denselayer13.norm1.weight\n",
      "features.denseblock4.denselayer13.norm1.bias\n",
      "features.denseblock4.denselayer13.norm1.running_mean\n",
      "features.denseblock4.denselayer13.norm1.running_var\n",
      "features.denseblock4.denselayer13.norm1.num_batches_tracked\n",
      "features.denseblock4.denselayer13.conv1.weight\n",
      "features.denseblock4.denselayer13.norm2.weight\n",
      "features.denseblock4.denselayer13.norm2.bias\n",
      "features.denseblock4.denselayer13.norm2.running_mean\n",
      "features.denseblock4.denselayer13.norm2.running_var\n",
      "features.denseblock4.denselayer13.norm2.num_batches_tracked\n",
      "features.denseblock4.denselayer13.conv2.weight\n",
      "features.denseblock4.denselayer14.norm1.weight\n",
      "features.denseblock4.denselayer14.norm1.bias\n",
      "features.denseblock4.denselayer14.norm1.running_mean\n",
      "features.denseblock4.denselayer14.norm1.running_var\n",
      "features.denseblock4.denselayer14.norm1.num_batches_tracked\n",
      "features.denseblock4.denselayer14.conv1.weight\n",
      "features.denseblock4.denselayer14.norm2.weight\n",
      "features.denseblock4.denselayer14.norm2.bias\n",
      "features.denseblock4.denselayer14.norm2.running_mean\n",
      "features.denseblock4.denselayer14.norm2.running_var\n",
      "features.denseblock4.denselayer14.norm2.num_batches_tracked\n",
      "features.denseblock4.denselayer14.conv2.weight\n",
      "features.denseblock4.denselayer15.norm1.weight\n",
      "features.denseblock4.denselayer15.norm1.bias\n",
      "features.denseblock4.denselayer15.norm1.running_mean\n",
      "features.denseblock4.denselayer15.norm1.running_var\n",
      "features.denseblock4.denselayer15.norm1.num_batches_tracked\n",
      "features.denseblock4.denselayer15.conv1.weight\n",
      "features.denseblock4.denselayer15.norm2.weight\n",
      "features.denseblock4.denselayer15.norm2.bias\n",
      "features.denseblock4.denselayer15.norm2.running_mean\n",
      "features.denseblock4.denselayer15.norm2.running_var\n",
      "features.denseblock4.denselayer15.norm2.num_batches_tracked\n",
      "features.denseblock4.denselayer15.conv2.weight\n",
      "features.denseblock4.denselayer16.norm1.weight\n",
      "features.denseblock4.denselayer16.norm1.bias\n",
      "features.denseblock4.denselayer16.norm1.running_mean\n",
      "features.denseblock4.denselayer16.norm1.running_var\n",
      "features.denseblock4.denselayer16.norm1.num_batches_tracked\n",
      "features.denseblock4.denselayer16.conv1.weight\n",
      "features.denseblock4.denselayer16.norm2.weight\n",
      "features.denseblock4.denselayer16.norm2.bias\n",
      "features.denseblock4.denselayer16.norm2.running_mean\n",
      "features.denseblock4.denselayer16.norm2.running_var\n",
      "features.denseblock4.denselayer16.norm2.num_batches_tracked\n",
      "features.denseblock4.denselayer16.conv2.weight\n",
      "features.norm5.weight\n",
      "features.norm5.bias\n",
      "features.norm5.running_mean\n",
      "features.norm5.running_var\n",
      "features.norm5.num_batches_tracked\n",
      "classifier.weight\n",
      "classifier.bias\n"
     ]
    }
   ],
   "source": [
    "for name in model.state_dict():\n",
    "#     if \"bias\" in name or \"norm\" in name or \"classifier\" in name:\n",
    "#         continue\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-05T08:50:27.505046Z",
     "start_time": "2020-07-05T08:50:27.482591Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()['features.denseblock1.denselayer1.norm1.num_batches_tracked']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-05T10:09:29.923699Z",
     "start_time": "2020-07-05T10:09:29.763617Z"
    }
   },
   "outputs": [],
   "source": [
    "def wcp_loss_torch(model, x, logit, epsilon=8., dr=0.5, num_simulations=1, xi=1e-6):\n",
    "    n_batch = x.shape[0]\n",
    "    size_list = [0]\n",
    "    size_sum = 0\n",
    "    for name in model.state_dict():\n",
    "        if \"bias\" in name or \"norm\" in name or \"classifier\" in name:\n",
    "            continue\n",
    "        size = model.state_dict()[name].numel()\n",
    "        size_sum += size \n",
    "        size_list += [size_sum]\n",
    "    d = xp.random.normal(size=x.shape)\n",
    "    d /= (1e-12 + xp.max(xp.abs(d),range(1, len(d.shape)), keepdims=True))\n",
    "    d /= xp.sqrt(1e-6 + xp.sum(d ** 2, range(1, len(d.shape)), keepdims=True))\n",
    "    d = from_dlpack(toDlpack(d)).type(torch.FloatTensor)\n",
    "    \n",
    "    d_weight = xp.random.normal(size=size_list[-1])\n",
    "    d_weight /= (1e-12 + xp.max(xp.abs(d_weight)))\n",
    "    d_weight /= xp.sqrt(1e-6 + xp.sum(d_weight ** 2))\n",
    "    d_weight = from_dlpack(toDlpack(d_weight)).type(torch.FloatTensor)\n",
    "    \n",
    "    drop_weight_list = []\n",
    "    for name in model.state_dict():\n",
    "        if \"bias\" in name or \"norm\" in name or \"classifier\" in name:\n",
    "            continue\n",
    "        drop_weight = xp.random.normal(size=model.state_dict()[name].numel() + 1)\n",
    "        drop_weight /= (1e-12 + xp.max(xp.abs(drop_weight)))\n",
    "        drop_weight /= xp.sqrt(1e-6 + xp.sum(drop_weight ** 2))\n",
    "        drop_weight = from_dlpack(toDlpack(drop_weight)).type(torch.FloatTensor)\n",
    "        drop_weight_list += [drop_weight]\n",
    "    print(\"d weight shape:\",d_weight.shape)\n",
    "    print(\"d shape:\",d.shape)\n",
    "    for _ in range(num_simulations):\n",
    "        x_d = xi * d\n",
    "        w_d = xi * d_weight\n",
    "        w_d = torch.ones(w_d.shape)\n",
    "        print(w_d.type())\n",
    "        x_d = Variable(x_d,requires_grad=True)\n",
    "        w_d = Variable(w_d,requires_grad=True)\n",
    "        w_d.retain_grad()\n",
    "        drop_d1_list = []\n",
    "        drop_d2_list = []\n",
    "        for ii in range(2):\n",
    "            drop_d1 = xi * (drop_weight_list[ii][:-1] + drop_weight_list[ii][-1])\n",
    "            drop_d2 = xi * drop_weight_list[ii][:-1]\n",
    "            drop_d1_list += [drop_d1]\n",
    "            drop_d2_list += [drop_d2] \n",
    "#         for name, module in model._modules.items():\n",
    "#             print(module.weight.data)\n",
    "#             module.weight.data = module.weight + w_d.reshape(module.weight.data.shape)\n",
    "#             print(module.weight.data)\n",
    "#             print(name)\n",
    "#             break\n",
    "        logit_d,d = delta_forward(model, logit, x, x_d, w_d, size_list)\n",
    "        logit_d,d = delta_forward_chainer(model, logit, x, x_d, w_d, size_list)\n",
    "        break\n",
    "        logit_drop1 = drop_forward(model, x, drop_d1_list)\n",
    "        logit_drop2 = drop_forward(model, x, drop_d2_list)       \n",
    "        #kl_loss = distance(logit.data, logit_d)\n",
    "        kl_loss = F.kl_div(F.softmax(logit),F.softmax(logit_d))\n",
    "#         kl_loss_drop1 = distance(logit.data, logit_drop1)\n",
    "#         kl_loss_drop2 = distance(logit.data, logit_drop2)\n",
    "        kl_loss_drop1 = F.kl_div(F.softmax(logit),F.softmax(logit_drop1))\n",
    "        kl_loss_drop2 = F.kl_div(F.softmax(logit),F.softmax(logit_drop2))\n",
    "        kl_loss.backward()\n",
    "        d,d_weight = x_d.grad,w_d.grad\n",
    "        model.zero_grad()\n",
    "        print(\"d:\",d)\n",
    "        print(\"d weight:\",d_weight)\n",
    "        break\n",
    "        d, d_weight = chainer.grad([kl_loss], [x_d, w_d], enable_double_backprop=False)\n",
    "        d = d / F.sqrt(F.sum(d ** 2, tuple(range(1, len(d.shape))), keepdims=True))\n",
    "        d_weight = d_weight / F.sqrt(F.sum(d_weight ** 2))\n",
    "        \n",
    "        layer1_drop1, layer4_drop1, layer7_drop1 = chainer.grad([kl_loss_drop1], drop_d1_list, enable_double_backprop=False)\n",
    "        layer1_drop2, layer4_drop2, layer7_drop2 = chainer.grad([kl_loss_drop2], drop_d2_list, enable_double_backprop=False)\n",
    "        \n",
    "        layer1_drop2 = F.reshape(F.sum(layer1_drop2), (1, 1))\n",
    "        layer4_drop2 = F.reshape(F.sum(layer4_drop2), (1, 1))\n",
    "        layer7_drop2 = F.reshape(F.sum(layer7_drop2), (1, 1))\n",
    "        drop_weight_list[0] = F.flatten(F.concat([F.reshape(layer1_drop1, (-1, 1)), layer1_drop2], axis=0))\n",
    "        drop_weight_list[1] = F.flatten(F.concat([F.reshape(layer4_drop1, (-1, 1)), layer4_drop2], axis=0))\n",
    "        drop_weight_list[2] = F.flatten(F.concat([F.reshape(layer7_drop1, (-1, 1)), layer7_drop2], axis=0))\n",
    "        drop_weight_list = [drop_weight_list[ii] / F.sqrt(F.sum(drop_weight_list[ii] ** 2)) for ii in range(3)]\n",
    "\n",
    "    return d,d_weight,drop_weight_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-05T11:48:10.930903Z",
     "start_time": "2020-07-05T11:48:10.805933Z"
    }
   },
   "outputs": [],
   "source": [
    "def delta_forward(model,logit, x, x_d, w_d, size_list):\n",
    "    h = x + x_d\n",
    "    # densenet\n",
    "    h = F.conv2d(h,model.state_dict()['features.conv0.weight'] + \n",
    "                 w_d[:size_list[1]].reshape(model.state_dict()['features.conv0.weight'].shape))\n",
    "    h = F.max_pool2d(F.relu(F.batch_norm(h,model.state_dict()['features.norm0.running_mean'],\n",
    "        model.state_dict()['features.norm0.running_var'],\n",
    "        model.state_dict()['features.norm0.weight'],\n",
    "        model.state_dict()['features.norm0.bias'])),kernel_size=3, stride=2, padding=1)\n",
    "    denselayer_num = [6,12,24,16]\n",
    "    k = 1\n",
    "    for i in range(1,5):\n",
    "        for j in range(1,denselayer_num[i]+1):\n",
    "            print(\"i:\",i)\n",
    "            print(\"j:\",j)\n",
    "        #denselayer 1\n",
    "            h_temp = h\n",
    "            dense_pre = 'features.denseblock'+str(i)+'.denselayer'+str(j)\n",
    "            h = F.relu(F.batch_norm(h,model.state_dict()[dense_pre+'.norm1.running_mean'],\n",
    "                model.state_dict()[dense_pre+'.norm1.running_var'],\n",
    "                model.state_dict()[dense_pre+'.norm1.weight'],\n",
    "                model.state_dict()[dense_pre+'.norm1.bias']))\n",
    "            h = F.conv2d(h,model.state_dict()[dense_pre+'.conv1.weight'] + \n",
    "                w_d[size_list[k]:size_list[k+1]].reshape(model.state_dict()[dense_pre+'.conv1.weight'].shape))\n",
    "            k+=1\n",
    "            h = F.relu(F.batch_norm(h,model.state_dict()[dense_pre+'.norm2.running_mean'],\n",
    "                model.state_dict()[dense_pre+'.norm2.running_var'],\n",
    "                model.state_dict()[dense_pre+'.norm2.weight'],\n",
    "                model.state_dict()[dense_pre+'.norm2.bias']))\n",
    "            h = F.conv2d(h,model.state_dict()[dense_pre+'.conv2.weight'] + \n",
    "                w_d[size_list[k]:size_list[k+1]].reshape(model.state_dict()[dense_pre+'.conv2.weight'].shape))\n",
    "            h = torch.cat([h_temp,h], 1)\n",
    "            k+=1\n",
    "        # transition1\n",
    "        if i < 4:\n",
    "            transi_pre = 'features.transition' + str(i)\n",
    "            h = F.relu(F.batch_norm(h,model.state_dict()[transi_pre+'.norm.running_mean'],\n",
    "                model.state_dict()[transi_pre+'.norm.running_var'],\n",
    "                model.state_dict()[transi_pre+'.norm.weight'],\n",
    "                model.state_dict()[transi_pre+'.norm.bias']))\n",
    "            h = F.conv2d(h,model.state_dict()[transi_pre+'.conv.weight'] + \n",
    "                w_d[size_list[k]:size_list[k+1]].reshape(model.state_dict()[transi_pre+'.conv.weight'].shape))\n",
    "            k+=1\n",
    "            h = F.avg_pool2d(h,kernel_size=2, stride=2)\n",
    "#     x = self.densenet121.features.conv0(x)\n",
    "    x = self.densenet121.features.norm0(x)\n",
    "    x = self.densenet121.features.relu0(x)\n",
    "    x = self.densenet121.features.pool0(x)\n",
    "    x = self.densenet121.features.denseblock1(x)\n",
    "    fea1 = self.densenet121.features.transition1(x)\n",
    "    fea2 = self.densenet121.features.denseblock2(fea1)\n",
    "    fea2 = self.densenet121.features.transition2(fea2)\n",
    "    fea3 = self.densenet121.features.denseblock3(fea2)\n",
    "    fea3 = self.densenet121.features.transition3(fea3)\n",
    "    \n",
    "    \n",
    "    h = F.relu(F.max_pool2d(F.conv2d(h,model.state_dict()['conv1.weight']+\n",
    "                                     w_d.reshape(model.state_dict()['conv1.weight'].shape),\n",
    "                                     model.state_dict()['conv1.bias']),2))\n",
    "    h = F.relu(F.max_pool2d(F.conv2d(h,model.state_dict()['conv2.weight'],\n",
    "                                     model.state_dict()['conv2.bias']),2))\n",
    "    h = h.view(-1,320)\n",
    "    h = F.relu(F.linear(h,model.state_dict()['fc1.weight'],model.state_dict()['fc1.bias']))\n",
    "    #x = F.dropout(x, training=self.training)\n",
    "    logit_perturb = F.linear(h,model.state_dict()['fc2.weight'],model.state_dict()['fc2.bias'])\n",
    "    p1 = model(x)\n",
    "    print(\"p1:\",p1)\n",
    "    print(\"p2:\",F.softmax(logit_perturb))\n",
    "    print(\"softmax:\",F.softmax(logit))\n",
    "    #kl_loss = distance(logit.detach().numpy(), logit_perturb.detach().numpy())\n",
    "    logp_hat = F.log_softmax(logit_perturb, dim=1)\n",
    "    kl_loss = F.kl_div(logp_hat,F.softmax(logit),reduction='batchmean')\n",
    "    print(\"kl loss torch:\",kl_loss)\n",
    "    kl_loss.backward()\n",
    "    d,d_weight = x_d.grad,w_d.grad\n",
    "    model.zero_grad()\n",
    "    print(\"d:\",d)\n",
    "    print(\"d weight:\",d_weight)\n",
    "    return d,d_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-05T11:48:16.090791Z",
     "start_time": "2020-07-05T11:48:13.607231Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('d weight shape:', torch.Size([6870208]))\n",
      "('d shape:', torch.Size([3, 3, 224, 224]))\n",
      "torch.FloatTensor\n",
      "('i:', 1)\n",
      "('j:', 1)\n",
      "('i:', 1)\n",
      "('j:', 2)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "running_mean should contain 32 elements not 96",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-85ed43690ded>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwcp_loss_torch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-39-9d1bce93cadf>\u001b[0m in \u001b[0;36mwcp_loss_torch\u001b[0;34m(model, x, logit, epsilon, dr, num_simulations, xi)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;31m#             print(name)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;31m#             break\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mlogit_d\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdelta_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0mlogit_d\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdelta_forward_chainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-48-e955ccf5cdc1>\u001b[0m in \u001b[0;36mdelta_forward\u001b[0;34m(model, logit, x, x_d, w_d, size_list)\u001b[0m\n\u001b[1;32m     19\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdense_pre\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.norm1.running_var'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdense_pre\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.norm1.weight'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m                 model.state_dict()[dense_pre+'.norm1.bias']))\n\u001b[0m\u001b[1;32m     22\u001b[0m             h = F.conv2d(h,model.state_dict()[dense_pre+'.conv1.weight'] + \n\u001b[1;32m     23\u001b[0m                 w_d[size_list[k]:size_list[k+1]].reshape(model.state_dict()[dense_pre+'.conv1.weight'].shape))\n",
      "\u001b[0;32m/home/luckie/anaconda2/envs/py2/lib/python2.7/site-packages/torch/nn/functional.pyc\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   1695\u001b[0m     return torch.batch_norm(\n\u001b[1;32m   1696\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1697\u001b[0;31m         \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1698\u001b[0m     )\n\u001b[1;32m   1699\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: running_mean should contain 32 elements not 96"
     ]
    }
   ],
   "source": [
    "_,_,output = wcp_loss_torch(model,image,out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "gist": {
   "data": {
    "description": "Untitled.ipynb",
    "public": false
   },
   "id": ""
  },
  "kernelspec": {
   "display_name": "Python [conda env:py2]",
   "language": "python",
   "name": "conda-env-py2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "272px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
